{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"A9Hpw7aIQrBw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":134},"outputId":"528cd211-c584-4f1b-cb3d-83523ba70cff","executionInfo":{"status":"error","timestamp":1574909669662,"user_tz":-540,"elapsed":1864,"user":{"displayName":"이주한","photoUrl":"","userId":"11677144677478737443"}}},"source":["\"# CNN 코드\\n\",\n","    \"\\n\",\n","    \"from __future__ import print_function\\n\",\n","    \"import argparse\\n\",\n","    \"import torch\\n\",\n","    \"import torch.nn as nn\\n\",\n","    \"import torch.nn.functional as F\\n\",\n","    \"import torch.optim as optim\\n\",\n","    \"from torchvision import datasets, transforms\\n\",\n","    \"from torch.autograd import Variable\\n\",\n","    \"\\n\",\n","    \"batch_size = 64\\n\",\n","    \"\\n\",\n","    \"train_dataset = datasets.MNIST(root='./data/',\\n\",\n","    \"                               train=True,\\n\",\n","    \"                               transform=transforms.ToTensor(),\\n\",\n","    \"                               download=True)\\n\",\n","    \"\\n\",\n","    \"test_dataset = datasets.MNIST(root='./data/',\\n\",\n","    \"                              train=False,\\n\",\n","    \"                              transform=transforms.ToTensor())\\n\",\n","    \"\\n\",\n","    \"train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\\n\",\n","    \"                                           batch_size=batch_size,\\n\",\n","    \"                                           shuffle=True)\\n\",\n","    \"\\n\",\n","    \"test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\\n\",\n","    \"                                          batch_size=batch_size,\\n\",\n","    \"                                          shuffle=False)\\n\",\n","    \"\\n\",\n","    \"\\n\",\n","    \"class Net(nn.Module):\\n\",\n","    \"\\n\",\n","    \"    def __init__(self):\\n\",\n","    \"        super(Net, self).__init__()\\n\",\n","    \"        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\\n\",\n","    \"        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\\n\",\n","    \"        self.mp = nn.MaxPool2d(2)\\n\",\n","    \"        self.fc = nn.Linear(320, 10)\\n\",\n","    \"\\n\",\n","    \"    def forward(self, x):\\n\",\n","    \"        in_size = x.size(0)\\n\",\n","    \"        x = F.relu(self.mp(self.conv1(x)))\\n\",\n","    \"        x = F.relu(self.mp(self.conv2(x)))\\n\",\n","    \"        x = x.view(in_size, -1)\\n\",\n","    \"        x = self.fc(x)\\n\",\n","    \"        return F.log_softmax(x)\\n\",\n","    \"\\n\",\n","    \"\\n\",\n","    \"model = Net()\\n\",\n","    \"optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\\n\",\n","    \"\\n\",\n","    \"def train(epoch):\\n\",\n","    \"    model.train()\\n\",\n","    \"    for batch_idx, (data, target) in enumerate(train_loader):\\n\",\n","    \"        data, target = Variable(data), Variable(target)\\n\",\n","    \"        optimizer.zero_grad()\\n\",\n","    \"        output = model(data)\\n\",\n","    \"        loss = F.nll_loss(output, target)\\n\",\n","    \"        loss.backward()\\n\",\n","    \"        optimizer.step()\\n\",\n","    \"        if batch_idx % 10 == 0:\\n\",\n","    \"            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\\\tLoss: {:.6f}'.format(\\n\",\n","    \"                epoch, batch_idx * len(data), len(train_loader.dataset),\\n\",\n","    \"                100. * batch_idx / len(train_loader), loss.item()))\\n\",\n","    \"            \\n\",\n","    \"def test():\\n\",\n","    \"    model.eval()\\n\",\n","    \"    test_loss = 0\\n\",\n","    \"    correct = 0\\n\",\n","    \"    for data, target in test_loader:\\n\",\n","    \"        data, target = Variable(data, volatile=True), Variable(target)\\n\",\n","    \"        output = model(data)\\n\",\n","    \"        # sum up batch loss\\n\",\n","    \"        test_loss += F.nll_loss(output, target, size_average=False).data\\n\",\n","    \"        # get the index of the max log-probability\\n\",\n","    \"        pred = output.data.max(1, keepdim=True)[1]\\n\",\n","    \"        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\\n\",\n","    \"\\n\",\n","    \"    test_loss /= len(test_loader.dataset)\\n\",\n","    \"    print('\\\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\\\n'.format(\\n\",\n","    \"        test_loss, correct, len(test_loader.dataset),\\n\",\n","    \"        100. * correct / len(test_loader.dataset)))\\n\",\n","    \"\\n\",\n","    \"\\n\",\n","    \"for epoch in range(1, 10):\\n\",\n","    \"    train(epoch)\\n\",\n","    \"    test()\"\n","   ]\n","  },\n","  {\n","   \"cell_type\": \"code\",\n","   \"execution_count\": 15,\n","   \"metadata\": {},\n","   \"outputs\": [\n","    {\n","     \"name\": \"stdout\",\n","     \"output_type\": \"stream\",\n","     \"text\": [\n","      \"train=25000 valid=25000\\n\",\n","      \"[('the', 322198), ('a', 159953), ('and', 158572), ('of', 144462), ('to', 133967), ('is', 104171), ('in', 90527), ('i', 70480), ('this', 69714), ('that', 66292)]\\n\",\n","      \"[(\\\"shite'\\\", 1), ('18,000', 1), ('whelk.<br', 1), ('chronicles.<br', 1), ('lascivious/decadent', 1), (\\\"me'...\\\", 1), ('american-canadian', 1), (\\\"'inferno'\\\", 1), (\\\"'irreversible'-style\\\", 1), ('intensity!', 1)]\\n\",\n","      \"vocab_size=10002\\n\",\n","      \"['<unk>', '<pad>', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'i']\\n\",\n","      \"dict_keys(['neg', 'pos'])\\n\",\n","      \"--------------------------------------------------------------------------------\\n\",\n","      \"model params\\n\",\n","      \"input_dim=10002, output=2\\n\",\n","      \"n_layers=1, n_hid=256 embed=100\\n\",\n","      \"batch=8\\n\",\n","      \"Epoch 0/10 loss=0.634299971575737 acc=0.755079984664917 time=0:21:20.982472\\n\",\n","      \"Epoch 1/10 loss=0.4012510151231289 acc=0.8662800192832947 time=0:18:50.133786\\n\"\n","     ]\n","    },\n","    {\n","     \"ename\": \"KeyboardInterrupt\",\n","     \"evalue\": \"\",\n","     \"output_type\": \"error\",\n","     \"traceback\": [\n","      \"\\u001b[1;31m---------------------------------------------------------------------------\\u001b[0m\",\n","      \"\\u001b[1;31mKeyboardInterrupt\\u001b[0m                         Traceback (most recent call last)\",\n","      \"\\u001b[1;32m<ipython-input-15-425d833d1ec9>\\u001b[0m in \\u001b[0;36m<module>\\u001b[1;34m\\u001b[0m\\n\\u001b[0;32m    171\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    172\\u001b[0m         \\u001b[1;31m# do back propagation for bptt steps in time\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m--> 173\\u001b[1;33m         \\u001b[0mloss\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mbackward\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[0;32m    174\\u001b[0m         \\u001b[0moptimizer\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mstep\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    175\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\",\n","      \"\\u001b[1;32m~\\\\Anaconda3\\\\envs\\\\test\\\\lib\\\\site-packages\\\\torch\\\\tensor.py\\u001b[0m in \\u001b[0;36mbackward\\u001b[1;34m(self, gradient, retain_graph, create_graph)\\u001b[0m\\n\\u001b[0;32m    116\\u001b[0m                 \\u001b[0mproducts\\u001b[0m\\u001b[1;33m.\\u001b[0m \\u001b[0mDefaults\\u001b[0m \\u001b[0mto\\u001b[0m\\u001b[0;31m \\u001b[0m\\u001b[0;31m`\\u001b[0m\\u001b[0;31m`\\u001b[0m\\u001b[1;32mFalse\\u001b[0m\\u001b[0;31m`\\u001b[0m\\u001b[0;31m`\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    117\\u001b[0m         \\\"\\\"\\\"\\n\\u001b[1;32m--> 118\\u001b[1;33m         \\u001b[0mtorch\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mautograd\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mbackward\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[0mself\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mgradient\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mretain_graph\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mcreate_graph\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[0;32m    119\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    120\\u001b[0m     \\u001b[1;32mdef\\u001b[0m \\u001b[0mregister_hook\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[0mself\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mhook\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m:\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\",\n","      \"\\u001b[1;32m~\\\\Anaconda3\\\\envs\\\\test\\\\lib\\\\site-packages\\\\torch\\\\autograd\\\\__init__.py\\u001b[0m in \\u001b[0;36mbackward\\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\\u001b[0m\\n\\u001b[0;32m     91\\u001b[0m     Variable._execution_engine.run_backward(\\n\\u001b[0;32m     92\\u001b[0m         \\u001b[0mtensors\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mgrad_tensors\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mretain_graph\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mcreate_graph\\u001b[0m\\u001b[1;33m,\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m---> 93\\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\\n\\u001b[0m\\u001b[0;32m     94\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m     95\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\",\n","      \"\\u001b[1;31mKeyboardInterrupt\\u001b[0m: \"\n","     ]\n","    }\n","   ]"],"execution_count":1,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-dc71dc8fc13d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \"\\n\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}]}]}