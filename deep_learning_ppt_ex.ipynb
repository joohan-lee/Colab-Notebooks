{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30168433.92405428\n",
      "1 24542104.87006099\n",
      "2 22253567.924964923\n",
      "3 19887953.997748602\n",
      "4 16300494.487796482\n",
      "5 12027108.305943988\n",
      "6 8086396.229705072\n",
      "7 5141974.102040187\n",
      "8 3222969.0547732995\n",
      "9 2064811.7455517296\n",
      "10 1384458.9957868294\n",
      "11 980968.9914972956\n",
      "12 733346.5184179352\n",
      "13 573659.4268765401\n",
      "14 464790.1919424674\n",
      "15 386328.72182872327\n",
      "16 327058.6671979148\n",
      "17 280609.86345606635\n",
      "18 243152.44109284287\n",
      "19 212258.11117653575\n",
      "20 186384.24447976163\n",
      "21 164449.3162485674\n",
      "22 145690.25940186734\n",
      "23 129507.52695405146\n",
      "24 115474.91968652813\n",
      "25 103249.57638227844\n",
      "26 92543.35994955277\n",
      "27 83137.87092980962\n",
      "28 74846.52103585168\n",
      "29 67520.57640839557\n",
      "30 61041.29141520457\n",
      "31 55283.06063068736\n",
      "32 50149.94703657917\n",
      "33 45566.86930750497\n",
      "34 41463.71408529411\n",
      "35 37783.25843333444\n",
      "36 34476.8386888496\n",
      "37 31500.62073614105\n",
      "38 28817.71290489302\n",
      "39 26393.08762342327\n",
      "40 24199.82399984905\n",
      "41 22212.96646858239\n",
      "42 20409.425111040597\n",
      "43 18771.571910760944\n",
      "44 17281.254441022604\n",
      "45 15923.544267780946\n",
      "46 14684.2932537786\n",
      "47 13554.217079666127\n",
      "48 12520.753894395664\n",
      "49 11575.128858901771\n",
      "50 10708.804188103288\n",
      "51 9913.937316791369\n",
      "52 9184.179102211\n",
      "53 8513.830953977944\n",
      "54 7897.287525329715\n",
      "55 7329.798361971334\n",
      "56 6806.973478751684\n",
      "57 6325.041749774778\n",
      "58 5880.488786971418\n",
      "59 5470.0129067401185\n",
      "60 5090.8276504975665\n",
      "61 4740.227793533762\n",
      "62 4415.925278031741\n",
      "63 4115.850714366465\n",
      "64 3838.2343476291444\n",
      "65 3580.9686242929974\n",
      "66 3342.3867799643394\n",
      "67 3121.057337807255\n",
      "68 2915.5584704673397\n",
      "69 2724.670897691776\n",
      "70 2547.331323755184\n",
      "71 2382.4651417851264\n",
      "72 2229.0657356054116\n",
      "73 2086.316697435918\n",
      "74 1953.3879252649213\n",
      "75 1829.5920507664796\n",
      "76 1714.205476897127\n",
      "77 1606.6549641851304\n",
      "78 1506.3610328692332\n",
      "79 1412.754744719448\n",
      "80 1325.3737871521641\n",
      "81 1243.7718769087382\n",
      "82 1167.5604198761955\n",
      "83 1096.3391072914922\n",
      "84 1029.756480196625\n",
      "85 967.4871911056172\n",
      "86 909.2256318980068\n",
      "87 854.7086110812049\n",
      "88 803.6738878035482\n",
      "89 755.8911199230188\n",
      "90 711.1131866896512\n",
      "91 669.1555147009892\n",
      "92 629.8396688712629\n",
      "93 592.9678107340692\n",
      "94 558.3947552260196\n",
      "95 525.9613839005324\n",
      "96 495.5335916272896\n",
      "97 466.9620600084891\n",
      "98 440.14446980408127\n",
      "99 414.95217408725875\n",
      "100 391.2877098308145\n",
      "101 369.059093305893\n",
      "102 348.1620286130472\n",
      "103 328.51419344691436\n",
      "104 310.040519301862\n",
      "105 292.661711364785\n",
      "106 276.31303777435346\n",
      "107 260.92876946997524\n",
      "108 246.44694962697264\n",
      "109 232.81522632028395\n",
      "110 219.97845965977672\n",
      "111 207.88820744827348\n",
      "112 196.50164642363598\n",
      "113 185.76970473948603\n",
      "114 175.65423519326487\n",
      "115 166.11975555205345\n",
      "116 157.13312980617354\n",
      "117 148.65452254283417\n",
      "118 140.65755356066524\n",
      "119 133.11362733305833\n",
      "120 125.99533513786875\n",
      "121 119.27752650395581\n",
      "122 112.93548919140763\n",
      "123 106.94770885615495\n",
      "124 101.29378610277504\n",
      "125 95.95375395551571\n",
      "126 90.90962338185935\n",
      "127 86.14390362669857\n",
      "128 81.63997803329127\n",
      "129 77.3830804668201\n",
      "130 73.35871285497669\n",
      "131 69.55444225653609\n",
      "132 65.95830402551351\n",
      "133 62.55597257918448\n",
      "134 59.336978626265164\n",
      "135 56.29145588367688\n",
      "136 53.40958258450567\n",
      "137 50.68236573922137\n",
      "138 48.10061427160118\n",
      "139 45.65646580912319\n",
      "140 43.34241434967848\n",
      "141 41.15120266605508\n",
      "142 39.07559550876924\n",
      "143 37.10985195290007\n",
      "144 35.24708423390898\n",
      "145 33.48188544019523\n",
      "146 31.808970530901533\n",
      "147 30.223356504399916\n",
      "148 28.72035489643504\n",
      "149 27.295951616494364\n",
      "150 25.94463160553957\n",
      "151 24.66318174512654\n",
      "152 23.44762252604682\n",
      "153 22.29455215541833\n",
      "154 21.200622906230127\n",
      "155 20.162606145755703\n",
      "156 19.17773387941552\n",
      "157 18.242815695898372\n",
      "158 17.355313674989134\n",
      "159 16.51273735371568\n",
      "160 15.71286523552273\n",
      "161 14.953218345632964\n",
      "162 14.231792694298388\n",
      "163 13.546653222538756\n",
      "164 12.895713699124112\n",
      "165 12.277307136231528\n",
      "166 11.689858351712973\n",
      "167 11.13165327406923\n",
      "168 10.600978130422304\n",
      "169 10.096572451101903\n",
      "170 9.617040807039759\n",
      "171 9.16132288286612\n",
      "172 8.727885993562314\n",
      "173 8.31569519428908\n",
      "174 7.923694248229507\n",
      "175 7.550879842850424\n",
      "176 7.19623141529509\n",
      "177 6.85881471547881\n",
      "178 6.537838096619764\n",
      "179 6.232375909350279\n",
      "180 5.941687280469576\n",
      "181 5.665042522001913\n",
      "182 5.40173847148528\n",
      "183 5.151068884870789\n",
      "184 4.912500844447726\n",
      "185 4.685364376170336\n",
      "186 4.4690592877355\n",
      "187 4.263069806505561\n",
      "188 4.066946904438771\n",
      "189 3.8801111345285846\n",
      "190 3.702123424245989\n",
      "191 3.532553790849576\n",
      "192 3.3710022069660104\n",
      "193 3.2170944546475413\n",
      "194 3.070415009362341\n",
      "195 2.930636547695819\n",
      "196 2.7974200012132657\n",
      "197 2.6704582780257997\n",
      "198 2.5494172365840577\n",
      "199 2.4340308389159904\n",
      "200 2.3240223826285344\n",
      "201 2.219155315052065\n",
      "202 2.119180668839732\n",
      "203 2.023798576552117\n",
      "204 1.9328349319328648\n",
      "205 1.846080833066489\n",
      "206 1.763334034493577\n",
      "207 1.6844021564384954\n",
      "208 1.6091036695563155\n",
      "209 1.5372566085761954\n",
      "210 1.4687107668888835\n",
      "211 1.4032982746998697\n",
      "212 1.3408755629638551\n",
      "213 1.2813008850035268\n",
      "214 1.2244453976153242\n",
      "215 1.170186337728408\n",
      "216 1.1183859396650075\n",
      "217 1.0689364222446185\n",
      "218 1.0217294931939978\n",
      "219 0.9766746958848853\n",
      "220 0.9336534289887144\n",
      "221 0.8925607451437365\n",
      "222 0.8533201157392667\n",
      "223 0.8158466243000295\n",
      "224 0.7800611204663429\n",
      "225 0.7458851559834174\n",
      "226 0.7132358303498509\n",
      "227 0.6820490109356141\n",
      "228 0.652257527140342\n",
      "229 0.6237968126276949\n",
      "230 0.5966175021391018\n",
      "231 0.5706392603513375\n",
      "232 0.5458150824103465\n",
      "233 0.5220939720988704\n",
      "234 0.49942630073937533\n",
      "235 0.477766459247765\n",
      "236 0.457073134734688\n",
      "237 0.4372866780218297\n",
      "238 0.418373144199216\n",
      "239 0.40029474297786033\n",
      "240 0.3830135990051335\n",
      "241 0.3664959404694835\n",
      "242 0.35070254146617824\n",
      "243 0.3356023720092033\n",
      "244 0.32116514898198784\n",
      "245 0.30736123482993233\n",
      "246 0.294162471381355\n",
      "247 0.28154185363817474\n",
      "248 0.26947123019272\n",
      "249 0.25792768355455437\n",
      "250 0.2468875885129721\n",
      "251 0.23632919947147743\n",
      "252 0.22623302992842514\n",
      "253 0.21657558489542472\n",
      "254 0.20733477821848872\n",
      "255 0.1984967531812784\n",
      "256 0.19004150909656314\n",
      "257 0.18195153654929408\n",
      "258 0.1742119419069501\n",
      "259 0.16680762116378\n",
      "260 0.15972218883188594\n",
      "261 0.15294289440910983\n",
      "262 0.14645590887759974\n",
      "263 0.14024818511815015\n",
      "264 0.13430788018158424\n",
      "265 0.12862381976896972\n",
      "266 0.12318338372548958\n",
      "267 0.11797651245240148\n",
      "268 0.11299410520599655\n",
      "269 0.10822498209335638\n",
      "270 0.10365924375194768\n",
      "271 0.09928929198988182\n",
      "272 0.09510638293993326\n",
      "273 0.09110168552478712\n",
      "274 0.08726797561610866\n",
      "275 0.08359787426295509\n",
      "276 0.08008425966814423\n",
      "277 0.0767204917897045\n",
      "278 0.07350015085567288\n",
      "279 0.07041645722336472\n",
      "280 0.0674637787281018\n",
      "281 0.0646365322879319\n",
      "282 0.06192940006318673\n",
      "283 0.05933701759235986\n",
      "284 0.05685617372552277\n",
      "285 0.054479064561230084\n",
      "286 0.05220222692821337\n",
      "287 0.05002183016323355\n",
      "288 0.04793352324978213\n",
      "289 0.045933419292866484\n",
      "290 0.04401774518968028\n",
      "291 0.04218293828603954\n",
      "292 0.04042580028120968\n",
      "293 0.038742396577480805\n",
      "294 0.03712986527794795\n",
      "295 0.03558518165553984\n",
      "296 0.034105490691520676\n",
      "297 0.03268801513683869\n",
      "298 0.03133012488084511\n",
      "299 0.03002975538027007\n",
      "300 0.028783539982463358\n",
      "301 0.027589434677475693\n",
      "302 0.02644541191637465\n",
      "303 0.025349317761615475\n",
      "304 0.024299077609800712\n",
      "305 0.023292789537214215\n",
      "306 0.02232869879573091\n",
      "307 0.02140484841010716\n",
      "308 0.02051956784991433\n",
      "309 0.01967123994335753\n",
      "310 0.01885830158123047\n",
      "311 0.01807928291900642\n",
      "312 0.01733274607955977\n",
      "313 0.016617397715979254\n",
      "314 0.01593207517685528\n",
      "315 0.015275069667965178\n",
      "316 0.0146453585527162\n",
      "317 0.014041784300215581\n",
      "318 0.013463313209424045\n",
      "319 0.0129088641890043\n",
      "320 0.012377443335214094\n",
      "321 0.011868146742737128\n",
      "322 0.011379936077099542\n",
      "323 0.010911970252922634\n",
      "324 0.01046339019905483\n",
      "325 0.010033397031019638\n",
      "326 0.009621218878895879\n",
      "327 0.00922611018155525\n",
      "328 0.008847460633368325\n",
      "329 0.00848447169780116\n",
      "330 0.008136389638937817\n",
      "331 0.0078026927964634386\n",
      "332 0.007482791014049393\n",
      "333 0.007176105659253136\n",
      "334 0.00688207173802464\n",
      "335 0.0066001751291157006\n",
      "336 0.006329925870967589\n",
      "337 0.0060708373596412785\n",
      "338 0.005822400775216998\n",
      "339 0.005584198132936788\n",
      "340 0.00535580775447099\n",
      "341 0.005136820063723773\n",
      "342 0.004926903263566373\n",
      "343 0.004725599872276726\n",
      "344 0.004532561117367685\n",
      "345 0.00434746072764643\n",
      "346 0.004169952846112421\n",
      "347 0.003999747732877961\n",
      "348 0.003836527743311677\n",
      "349 0.0036800087525877233\n",
      "350 0.00352995428280687\n",
      "351 0.0033860204214373607\n",
      "352 0.003248004583370698\n",
      "353 0.003115644862452368\n",
      "354 0.0029887001429039226\n",
      "355 0.0028669580851070804\n",
      "356 0.0027502497696551996\n",
      "357 0.0026382803675840663\n",
      "358 0.0025308922599611905\n",
      "359 0.002427899999822154\n",
      "360 0.002329128059322754\n",
      "361 0.002234404721901866\n",
      "362 0.002143542941734499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363 0.002056395427440762\n",
      "364 0.001972810486675389\n",
      "365 0.0018926421471034106\n",
      "366 0.0018157512546645494\n",
      "367 0.0017419969530487504\n",
      "368 0.0016712559483561115\n",
      "369 0.0016034291378968888\n",
      "370 0.0015383488055836629\n",
      "371 0.0014759178655601509\n",
      "372 0.0014160321874857195\n",
      "373 0.001358588455702754\n",
      "374 0.0013034868260092239\n",
      "375 0.001250633230681981\n",
      "376 0.001199930752997159\n",
      "377 0.0011512997440763656\n",
      "378 0.0011046438719663893\n",
      "379 0.001059887424574844\n",
      "380 0.0010169533026275677\n",
      "381 0.0009757652923519574\n",
      "382 0.0009362641601637723\n",
      "383 0.0008983609055467587\n",
      "384 0.0008619981618747696\n",
      "385 0.0008271155436032195\n",
      "386 0.0007936521536810561\n",
      "387 0.0007615504123197028\n",
      "388 0.000730747204556138\n",
      "389 0.000701195871797739\n",
      "390 0.0006728436268438736\n",
      "391 0.0006456425490691464\n",
      "392 0.0006195457325254176\n",
      "393 0.0005945096189781545\n",
      "394 0.0005704919026617194\n",
      "395 0.0005474517177356134\n",
      "396 0.0005253388995283829\n",
      "397 0.0005041232798126631\n",
      "398 0.000483767957370306\n",
      "399 0.00046423706246659173\n",
      "400 0.0004454977240680871\n",
      "401 0.0004275177993012841\n",
      "402 0.00041026861946622103\n",
      "403 0.0003937168261812746\n",
      "404 0.0003778343176673457\n",
      "405 0.0003625947738618307\n",
      "406 0.00034797252148492826\n",
      "407 0.0003339439573432343\n",
      "408 0.000320483534164313\n",
      "409 0.0003075649979097007\n",
      "410 0.0002951693804001818\n",
      "411 0.0002832770707577049\n",
      "412 0.00027186334343933975\n",
      "413 0.00026091107999758414\n",
      "414 0.00025040157872841017\n",
      "415 0.00024031711823054502\n",
      "416 0.0002306400589900352\n",
      "417 0.00022135385582250323\n",
      "418 0.00021244271180299556\n",
      "419 0.0002038922186168492\n",
      "420 0.0001956899561202347\n",
      "421 0.00018781516399149451\n",
      "422 0.0001802582881598344\n",
      "423 0.0001730064831667287\n",
      "424 0.00016604757719132068\n",
      "425 0.00015936922010111629\n",
      "426 0.00015296104643077385\n",
      "427 0.0001468113751160096\n",
      "428 0.00014090974707277028\n",
      "429 0.00013524561178813218\n",
      "430 0.00012980953681945788\n",
      "431 0.00012459258538005257\n",
      "432 0.00011958722604563159\n",
      "433 0.0001147827735071584\n",
      "434 0.0001101714241149929\n",
      "435 0.00010574588452104205\n",
      "436 0.00010149867760434077\n",
      "437 9.742316623428702e-05\n",
      "438 9.351112095904496e-05\n",
      "439 8.975655341212919e-05\n",
      "440 8.615315167215959e-05\n",
      "441 8.269493947713537e-05\n",
      "442 7.937586794112601e-05\n",
      "443 7.619029431213571e-05\n",
      "444 7.313381950737935e-05\n",
      "445 7.019967030297263e-05\n",
      "446 6.738367425445078e-05\n",
      "447 6.46805833383151e-05\n",
      "448 6.208619885332149e-05\n",
      "449 5.959615324710716e-05\n",
      "450 5.720632476481385e-05\n",
      "451 5.4912467340547255e-05\n",
      "452 5.271084166963338e-05\n",
      "453 5.059768192223452e-05\n",
      "454 4.85696946266692e-05\n",
      "455 4.662328590887392e-05\n",
      "456 4.4755163005187894e-05\n",
      "457 4.296168125692491e-05\n",
      "458 4.124029168367437e-05\n",
      "459 3.958801801387291e-05\n",
      "460 3.800206388520414e-05\n",
      "461 3.6479795575421305e-05\n",
      "462 3.5018691093715264e-05\n",
      "463 3.361639934159321e-05\n",
      "464 3.22702406419053e-05\n",
      "465 3.097807396703023e-05\n",
      "466 2.973777413201338e-05\n",
      "467 2.8547608641106803e-05\n",
      "468 2.74051482949227e-05\n",
      "469 2.6308190953631416e-05\n",
      "470 2.5255241948807216e-05\n",
      "471 2.4244578065125956e-05\n",
      "472 2.3274541204928855e-05\n",
      "473 2.2343258118215393e-05\n",
      "474 2.1449320941201276e-05\n",
      "475 2.059125919144538e-05\n",
      "476 1.9767564104053666e-05\n",
      "477 1.8976888362650165e-05\n",
      "478 1.8217967867655977e-05\n",
      "479 1.748957614460359e-05\n",
      "480 1.6790206865714317e-05\n",
      "481 1.611893926781456e-05\n",
      "482 1.5474471078851662e-05\n",
      "483 1.4855845521498452e-05\n",
      "484 1.4261983543820475e-05\n",
      "485 1.3691902774192243e-05\n",
      "486 1.314465577258197e-05\n",
      "487 1.2619336744919664e-05\n",
      "488 1.2115045024132146e-05\n",
      "489 1.1630946746001336e-05\n",
      "490 1.1166419816104041e-05\n",
      "491 1.0720300751815795e-05\n",
      "492 1.0292033222721702e-05\n",
      "493 9.880898097142856e-06\n",
      "494 9.486220358243078e-06\n",
      "495 9.107343693845042e-06\n",
      "496 8.743624474582773e-06\n",
      "497 8.394450824612984e-06\n",
      "498 8.059254947468361e-06\n",
      "499 7.737505959667637e-06\n"
     ]
    }
   ],
   "source": [
    "#Numpy를 사용한 신경망 구성(ppt 12p)\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36260556.0\n",
      "1 39129888.0\n",
      "2 45526988.0\n",
      "3 44810464.0\n",
      "4 32529744.0\n",
      "5 16608294.0\n",
      "6 6963236.0\n",
      "7 3131073.25\n",
      "8 1810432.125\n",
      "9 1290916.25\n",
      "10 1023152.0\n",
      "11 847727.4375\n",
      "12 716465.25\n",
      "13 611981.25\n",
      "14 526731.5625\n",
      "15 456156.4375\n",
      "16 397104.0625\n",
      "17 347331.4375\n",
      "18 305121.28125\n",
      "19 269080.125\n",
      "20 238143.921875\n",
      "21 211443.796875\n",
      "22 188309.421875\n",
      "23 168210.84375\n",
      "24 150660.375\n",
      "25 135265.640625\n",
      "26 121727.9453125\n",
      "27 109778.8046875\n",
      "28 99205.140625\n",
      "29 89824.6875\n",
      "30 81475.7890625\n",
      "31 74044.421875\n",
      "32 67407.90625\n",
      "33 61464.8515625\n",
      "34 56125.24609375\n",
      "35 51320.984375\n",
      "36 46988.78515625\n",
      "37 43077.2265625\n",
      "38 39540.33203125\n",
      "39 36338.3359375\n",
      "40 33434.5078125\n",
      "41 30796.5078125\n",
      "42 28396.865234375\n",
      "43 26209.66015625\n",
      "44 24214.37890625\n",
      "45 22390.96484375\n",
      "46 20723.89453125\n",
      "47 19196.826171875\n",
      "48 17797.6796875\n",
      "49 16513.60546875\n",
      "50 15335.0703125\n",
      "51 14251.375\n",
      "52 13254.0205078125\n",
      "53 12334.8896484375\n",
      "54 11487.71875\n",
      "55 10705.5224609375\n",
      "56 9983.27734375\n",
      "57 9315.224609375\n",
      "58 8696.685546875\n",
      "59 8123.7431640625\n",
      "60 7592.49609375\n",
      "61 7099.76171875\n",
      "62 6642.5595703125\n",
      "63 6217.7431640625\n",
      "64 5823.271484375\n",
      "65 5456.0634765625\n",
      "66 5114.24755859375\n",
      "67 4795.9052734375\n",
      "68 4499.279296875\n",
      "69 4222.6044921875\n",
      "70 3964.505615234375\n",
      "71 3723.55615234375\n",
      "72 3498.498046875\n",
      "73 3288.22412109375\n",
      "74 3091.6357421875\n",
      "75 2907.788818359375\n",
      "76 2735.85791015625\n",
      "77 2574.913818359375\n",
      "78 2424.092041015625\n",
      "79 2282.835693359375\n",
      "80 2150.396484375\n",
      "81 2026.22265625\n",
      "82 1909.771728515625\n",
      "83 1800.4969482421875\n",
      "84 1697.9229736328125\n",
      "85 1601.633544921875\n",
      "86 1511.2020263671875\n",
      "87 1426.23779296875\n",
      "88 1346.39599609375\n",
      "89 1271.344970703125\n",
      "90 1200.733154296875\n",
      "91 1134.323974609375\n",
      "92 1071.8616943359375\n",
      "93 1013.0151977539062\n",
      "94 957.6077270507812\n",
      "95 905.4227294921875\n",
      "96 856.2552490234375\n",
      "97 809.9348754882812\n",
      "98 766.2664794921875\n",
      "99 725.0896606445312\n",
      "100 686.285888671875\n",
      "101 649.7119140625\n",
      "102 615.2003784179688\n",
      "103 582.6201782226562\n",
      "104 551.8648681640625\n",
      "105 522.828125\n",
      "106 495.39923095703125\n",
      "107 469.4888916015625\n",
      "108 445.0103759765625\n",
      "109 421.8802185058594\n",
      "110 400.0161437988281\n",
      "111 379.34033203125\n",
      "112 359.7861022949219\n",
      "113 341.2943420410156\n",
      "114 323.80364990234375\n",
      "115 307.25152587890625\n",
      "116 291.58721923828125\n",
      "117 276.7630310058594\n",
      "118 262.7290344238281\n",
      "119 249.4404296875\n",
      "120 236.8592529296875\n",
      "121 224.9392547607422\n",
      "122 213.64581298828125\n",
      "123 202.94703674316406\n",
      "124 192.81007385253906\n",
      "125 183.2013397216797\n",
      "126 174.09449768066406\n",
      "127 165.45933532714844\n",
      "128 157.27154541015625\n",
      "129 149.50721740722656\n",
      "130 142.14306640625\n",
      "131 135.15756225585938\n",
      "132 128.53038024902344\n",
      "133 122.24209594726562\n",
      "134 116.27169799804688\n",
      "135 110.6077880859375\n",
      "136 105.2304458618164\n",
      "137 100.12413024902344\n",
      "138 95.27896881103516\n",
      "139 90.67515563964844\n",
      "140 86.30326843261719\n",
      "141 82.15062713623047\n",
      "142 78.20487213134766\n",
      "143 74.45803833007812\n",
      "144 70.89495849609375\n",
      "145 67.51004028320312\n",
      "146 64.29293823242188\n",
      "147 61.2352180480957\n",
      "148 58.327579498291016\n",
      "149 55.56414031982422\n",
      "150 52.93621826171875\n",
      "151 50.437110900878906\n",
      "152 48.0598258972168\n",
      "153 45.79854965209961\n",
      "154 43.64772033691406\n",
      "155 41.60176467895508\n",
      "156 39.655487060546875\n",
      "157 37.80290222167969\n",
      "158 36.03941345214844\n",
      "159 34.362030029296875\n",
      "160 32.76531982421875\n",
      "161 31.244098663330078\n",
      "162 29.796274185180664\n",
      "163 28.418203353881836\n",
      "164 27.105749130249023\n",
      "165 25.856029510498047\n",
      "166 24.66580581665039\n",
      "167 23.531620025634766\n",
      "168 22.45168685913086\n",
      "169 21.423053741455078\n",
      "170 20.442522048950195\n",
      "171 19.508834838867188\n",
      "172 18.618343353271484\n",
      "173 17.77013397216797\n",
      "174 16.96196746826172\n",
      "175 16.19123077392578\n",
      "176 15.456826210021973\n",
      "177 14.756632804870605\n",
      "178 14.089095115661621\n",
      "179 13.452820777893066\n",
      "180 12.846048355102539\n",
      "181 12.267173767089844\n",
      "182 11.71505355834961\n",
      "183 11.18903923034668\n",
      "184 10.686959266662598\n",
      "185 10.207862854003906\n",
      "186 9.751106262207031\n",
      "187 9.315065383911133\n",
      "188 8.8995361328125\n",
      "189 8.502388954162598\n",
      "190 8.124067306518555\n",
      "191 7.76298713684082\n",
      "192 7.418051719665527\n",
      "193 7.088992118835449\n",
      "194 6.77480936050415\n",
      "195 6.475375652313232\n",
      "196 6.188958168029785\n",
      "197 5.915542125701904\n",
      "198 5.654609680175781\n",
      "199 5.405423164367676\n",
      "200 5.167613506317139\n",
      "201 4.940393447875977\n",
      "202 4.723442077636719\n",
      "203 4.516409397125244\n",
      "204 4.318521499633789\n",
      "205 4.129589080810547\n",
      "206 3.948823928833008\n",
      "207 3.7762410640716553\n",
      "208 3.6116349697113037\n",
      "209 3.4542903900146484\n",
      "210 3.303877830505371\n",
      "211 3.1602516174316406\n",
      "212 3.0226798057556152\n",
      "213 2.891688823699951\n",
      "214 2.7662887573242188\n",
      "215 2.646548271179199\n",
      "216 2.531839370727539\n",
      "217 2.422600030899048\n",
      "218 2.317808151245117\n",
      "219 2.2176880836486816\n",
      "220 2.1221559047698975\n",
      "221 2.03068470954895\n",
      "222 1.9433361291885376\n",
      "223 1.8596689701080322\n",
      "224 1.779785394668579\n",
      "225 1.703394889831543\n",
      "226 1.630456805229187\n",
      "227 1.560450553894043\n",
      "228 1.4936332702636719\n",
      "229 1.429755687713623\n",
      "230 1.3686789274215698\n",
      "231 1.3102446794509888\n",
      "232 1.2543052434921265\n",
      "233 1.2009013891220093\n",
      "234 1.149824619293213\n",
      "235 1.100847601890564\n",
      "236 1.0540707111358643\n",
      "237 1.009291410446167\n",
      "238 0.9664108157157898\n",
      "239 0.9253896474838257\n",
      "240 0.8860939741134644\n",
      "241 0.8486550450325012\n",
      "242 0.8126612305641174\n",
      "243 0.7783626317977905\n",
      "244 0.7454854846000671\n",
      "245 0.7139960527420044\n",
      "246 0.683870792388916\n",
      "247 0.6550054550170898\n",
      "248 0.6274755001068115\n",
      "249 0.6010075807571411\n",
      "250 0.5756260752677917\n",
      "251 0.5514483451843262\n",
      "252 0.5283670425415039\n",
      "253 0.5061655044555664\n",
      "254 0.4848988652229309\n",
      "255 0.46455568075180054\n",
      "256 0.4451426863670349\n",
      "257 0.42647790908813477\n",
      "258 0.408628910779953\n",
      "259 0.39150890707969666\n",
      "260 0.3751547634601593\n",
      "261 0.3595108389854431\n",
      "262 0.34446239471435547\n",
      "263 0.3301142156124115\n",
      "264 0.3163734972476959\n",
      "265 0.3031875491142273\n",
      "266 0.2905411720275879\n",
      "267 0.2784351706504822\n",
      "268 0.2668836712837219\n",
      "269 0.25578275322914124\n",
      "270 0.2451336532831192\n",
      "271 0.234996497631073\n",
      "272 0.22519315779209137\n",
      "273 0.21590843796730042\n",
      "274 0.2069994956254959\n",
      "275 0.19840484857559204\n",
      "276 0.19017232954502106\n",
      "277 0.1823490560054779\n",
      "278 0.17477957904338837\n",
      "279 0.16753657162189484\n",
      "280 0.16064321994781494\n",
      "281 0.1540313959121704\n",
      "282 0.14764317870140076\n",
      "283 0.14155642688274384\n",
      "284 0.13575850427150726\n",
      "285 0.1301533579826355\n",
      "286 0.12475703656673431\n",
      "287 0.11963947862386703\n",
      "288 0.11474557220935822\n",
      "289 0.11002309620380402\n",
      "290 0.10549414902925491\n",
      "291 0.10114670544862747\n",
      "292 0.09699735790491104\n",
      "293 0.09303834289312363\n",
      "294 0.08920656889677048\n",
      "295 0.08556753396987915\n",
      "296 0.08205338567495346\n",
      "297 0.0787029042840004\n",
      "298 0.0754944235086441\n",
      "299 0.0724039152264595\n",
      "300 0.06944087892770767\n",
      "301 0.06658989191055298\n",
      "302 0.06387536227703094\n",
      "303 0.06128348410129547\n",
      "304 0.058775562793016434\n",
      "305 0.05638016387820244\n",
      "306 0.05409037694334984\n",
      "307 0.05188797041773796\n",
      "308 0.049769867211580276\n",
      "309 0.04775405675172806\n",
      "310 0.045802585780620575\n",
      "311 0.043956078588962555\n",
      "312 0.04217488318681717\n",
      "313 0.04045863822102547\n",
      "314 0.03883267194032669\n",
      "315 0.037250544875860214\n",
      "316 0.03572699800133705\n",
      "317 0.034289274364709854\n",
      "318 0.03288355842232704\n",
      "319 0.031569141894578934\n",
      "320 0.03030262514948845\n",
      "321 0.029074300080537796\n",
      "322 0.02790464088320732\n",
      "323 0.02677157148718834\n",
      "324 0.025715596973896027\n",
      "325 0.024670619517564774\n",
      "326 0.02368767000734806\n",
      "327 0.022738808766007423\n",
      "328 0.021824657917022705\n",
      "329 0.020948611199855804\n",
      "330 0.020107552409172058\n",
      "331 0.01931091398000717\n",
      "332 0.01854720525443554\n",
      "333 0.017808277159929276\n",
      "334 0.017098145559430122\n",
      "335 0.016415968537330627\n",
      "336 0.015766456723213196\n",
      "337 0.015149040147662163\n",
      "338 0.014546379446983337\n",
      "339 0.013975714333355427\n",
      "340 0.013424449600279331\n",
      "341 0.012896673753857613\n",
      "342 0.012388273142278194\n",
      "343 0.0118998559191823\n",
      "344 0.011434405110776424\n",
      "345 0.010989649221301079\n",
      "346 0.010567829012870789\n",
      "347 0.010152192786335945\n",
      "348 0.00975760817527771\n",
      "349 0.009376352652907372\n",
      "350 0.009013483300805092\n",
      "351 0.008661432191729546\n",
      "352 0.008331317454576492\n",
      "353 0.008008004166185856\n",
      "354 0.007699537090957165\n",
      "355 0.007407390046864748\n",
      "356 0.00713490741327405\n",
      "357 0.006861978676170111\n",
      "358 0.006597853731364012\n",
      "359 0.006349863018840551\n",
      "360 0.006109384819865227\n",
      "361 0.005882015451788902\n",
      "362 0.005659968126565218\n",
      "363 0.005451054312288761\n",
      "364 0.005250079557299614\n",
      "365 0.00505339028313756\n",
      "366 0.004864105023443699\n",
      "367 0.004688305780291557\n",
      "368 0.004516054410487413\n",
      "369 0.004345317371189594\n",
      "370 0.004187015816569328\n",
      "371 0.00403589429333806\n",
      "372 0.003891177475452423\n",
      "373 0.0037507889792323112\n",
      "374 0.003615111578255892\n",
      "375 0.003483179723843932\n",
      "376 0.003361885203048587\n",
      "377 0.003243360435590148\n",
      "378 0.0031280959956347942\n",
      "379 0.0030165102798491716\n",
      "380 0.002911556279286742\n",
      "381 0.0028104486409574747\n",
      "382 0.002713653491809964\n",
      "383 0.002620065351948142\n",
      "384 0.0025296160019934177\n",
      "385 0.002441643038764596\n",
      "386 0.0023589979391545057\n",
      "387 0.002279197331517935\n",
      "388 0.0022025159560143948\n",
      "389 0.00212572212330997\n",
      "390 0.0020537246018648148\n",
      "391 0.0019870914984494448\n",
      "392 0.00191968004219234\n",
      "393 0.0018547950312495232\n",
      "394 0.0017956251977011561\n",
      "395 0.001737295649945736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396 0.0016821601893752813\n",
      "397 0.0016254652291536331\n",
      "398 0.0015749476151540875\n",
      "399 0.001525234431028366\n",
      "400 0.001474960707128048\n",
      "401 0.0014298749156296253\n",
      "402 0.001385753508657217\n",
      "403 0.0013418745948001742\n",
      "404 0.0013026765082031488\n",
      "405 0.0012617744505405426\n",
      "406 0.0012232098961248994\n",
      "407 0.001185207162052393\n",
      "408 0.0011495959479361773\n",
      "409 0.00111553561873734\n",
      "410 0.001084539107978344\n",
      "411 0.0010527996346354485\n",
      "412 0.0010217740200459957\n",
      "413 0.0009924639016389847\n",
      "414 0.0009633276495151222\n",
      "415 0.0009351374465040863\n",
      "416 0.0009078881703317165\n",
      "417 0.0008822624222375453\n",
      "418 0.0008572190417908132\n",
      "419 0.0008352932636626065\n",
      "420 0.0008107650792226195\n",
      "421 0.0007888434338383377\n",
      "422 0.0007675255183130503\n",
      "423 0.0007464329246431589\n",
      "424 0.0007256073877215385\n",
      "425 0.0007063083467073739\n",
      "426 0.0006886015180498362\n",
      "427 0.0006695838528685272\n",
      "428 0.0006506362115032971\n",
      "429 0.0006343303248286247\n",
      "430 0.0006181694334372878\n",
      "431 0.0006007730262354016\n",
      "432 0.0005862939287908375\n",
      "433 0.0005718710599467158\n",
      "434 0.0005572953959926963\n",
      "435 0.0005435872590169311\n",
      "436 0.0005287673557177186\n",
      "437 0.0005155625403858721\n",
      "438 0.0005029535968787968\n",
      "439 0.0004905256209895015\n",
      "440 0.0004775211273226887\n",
      "441 0.0004661163256969303\n",
      "442 0.0004551276797428727\n",
      "443 0.000445326033513993\n",
      "444 0.0004341180028859526\n",
      "445 0.00042441030382178724\n",
      "446 0.0004144253907725215\n",
      "447 0.0004048213013447821\n",
      "448 0.0003945887729059905\n",
      "449 0.0003853219677694142\n",
      "450 0.00037664288538508117\n",
      "451 0.00036784724215976894\n",
      "452 0.0003593863220885396\n",
      "453 0.00035167898749932647\n",
      "454 0.0003438110288698226\n",
      "455 0.000335585413267836\n",
      "456 0.0003284734848421067\n",
      "457 0.00032101289252750576\n",
      "458 0.0003137140301987529\n",
      "459 0.00030738444183953106\n",
      "460 0.0003006532497238368\n",
      "461 0.00029432083829306066\n",
      "462 0.00028831756208091974\n",
      "463 0.0002818580833263695\n",
      "464 0.000275815516943112\n",
      "465 0.00026932492619380355\n",
      "466 0.0002636097779031843\n",
      "467 0.0002576723345555365\n",
      "468 0.00025256266235373914\n",
      "469 0.0002475631481502205\n",
      "470 0.00024256824690382928\n",
      "471 0.0002373621246078983\n",
      "472 0.00023250783851835877\n",
      "473 0.0002278225147165358\n",
      "474 0.00022321962751448154\n",
      "475 0.00021896776161156595\n",
      "476 0.00021477478730957955\n",
      "477 0.00021029001800343394\n",
      "478 0.00020625104662030935\n",
      "479 0.0002023133129114285\n",
      "480 0.00019853156118188053\n",
      "481 0.0001942029339261353\n",
      "482 0.0001907559926621616\n",
      "483 0.00018708279822021723\n",
      "484 0.00018402058049105108\n",
      "485 0.0001810635149013251\n",
      "486 0.00017691291577648371\n",
      "487 0.0001739754807204008\n",
      "488 0.00017096239025704563\n",
      "489 0.00016731400683056563\n",
      "490 0.00016396680439356714\n",
      "491 0.00016119147767312825\n",
      "492 0.0001582812110427767\n",
      "493 0.00015549655654467642\n",
      "494 0.00015249740681611001\n",
      "495 0.00014983295113779604\n",
      "496 0.00014736966113559902\n",
      "497 0.00014482540427707136\n",
      "498 0.00014212142559699714\n",
      "499 0.000139645257149823\n"
     ]
    }
   ],
   "source": [
    "#Tensor를 사용한 신경망 구성(ppt 13p)\n",
    "# -*- coding: utf-8 \n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33879552.0\n",
      "1 35144328.0\n",
      "2 40267632.0\n",
      "3 40891788.0\n",
      "4 32356498.0\n",
      "5 18696562.0\n",
      "6 8624855.0\n",
      "7 3816730.0\n",
      "8 1964986.375\n",
      "9 1244585.375\n",
      "10 916673.0625\n",
      "11 730944.5\n",
      "12 604831.3125\n",
      "13 509687.15625\n",
      "14 434290.8125\n",
      "15 372886.65625\n",
      "16 322072.59375\n",
      "17 279710.90625\n",
      "18 244114.78125\n",
      "19 214004.71875\n",
      "20 188347.859375\n",
      "21 166370.53125\n",
      "22 147458.21875\n",
      "23 131123.734375\n",
      "24 116949.5078125\n",
      "25 104620.6015625\n",
      "26 93854.0390625\n",
      "27 84406.953125\n",
      "28 76094.53125\n",
      "29 68764.5390625\n",
      "30 62280.98828125\n",
      "31 56516.48046875\n",
      "32 51379.77734375\n",
      "33 46792.7109375\n",
      "34 42687.671875\n",
      "35 39006.140625\n",
      "36 35692.80078125\n",
      "37 32705.296875\n",
      "38 30005.3203125\n",
      "39 27561.37109375\n",
      "40 25347.3203125\n",
      "41 23338.005859375\n",
      "42 21510.521484375\n",
      "43 19845.275390625\n",
      "44 18326.693359375\n",
      "45 16940.33984375\n",
      "46 15672.216796875\n",
      "47 14510.80859375\n",
      "48 13445.5390625\n",
      "49 12467.2822265625\n",
      "50 11568.4853515625\n",
      "51 10741.572265625\n",
      "52 9980.7197265625\n",
      "53 9278.5986328125\n",
      "54 8631.2548828125\n",
      "55 8033.6552734375\n",
      "56 7481.1171875\n",
      "57 6970.32958984375\n",
      "58 6497.51171875\n",
      "59 6059.79150390625\n",
      "60 5654.0322265625\n",
      "61 5277.7626953125\n",
      "62 4928.82421875\n",
      "63 4604.7919921875\n",
      "64 4303.75439453125\n",
      "65 4023.921875\n",
      "66 3763.87060546875\n",
      "67 3522.111328125\n",
      "68 3296.903076171875\n",
      "69 3087.14013671875\n",
      "70 2891.72900390625\n",
      "71 2709.588134765625\n",
      "72 2539.737060546875\n",
      "73 2381.109619140625\n",
      "74 2233.1142578125\n",
      "75 2094.9873046875\n",
      "76 1965.873779296875\n",
      "77 1845.23779296875\n",
      "78 1732.412841796875\n",
      "79 1627.001220703125\n",
      "80 1528.344482421875\n",
      "81 1435.9957275390625\n",
      "82 1349.599853515625\n",
      "83 1268.714599609375\n",
      "84 1192.923828125\n",
      "85 1121.87109375\n",
      "86 1055.2706298828125\n",
      "87 992.8850708007812\n",
      "88 934.3853149414062\n",
      "89 879.4932861328125\n",
      "90 827.9854736328125\n",
      "91 779.6583862304688\n",
      "92 734.292724609375\n",
      "93 691.6989135742188\n",
      "94 651.6887817382812\n",
      "95 614.10498046875\n",
      "96 578.8509521484375\n",
      "97 545.7431640625\n",
      "98 514.6044311523438\n",
      "99 485.35064697265625\n",
      "100 457.82196044921875\n",
      "101 431.91656494140625\n",
      "102 407.58038330078125\n",
      "103 384.6416015625\n",
      "104 363.05340576171875\n",
      "105 342.7299499511719\n",
      "106 323.5890808105469\n",
      "107 305.56561279296875\n",
      "108 288.5841979980469\n",
      "109 272.5788879394531\n",
      "110 257.50738525390625\n",
      "111 243.29148864746094\n",
      "112 229.89170837402344\n",
      "113 217.26626586914062\n",
      "114 205.36077880859375\n",
      "115 194.12872314453125\n",
      "116 183.52853393554688\n",
      "117 173.5308837890625\n",
      "118 164.1000518798828\n",
      "119 155.191650390625\n",
      "120 146.78887939453125\n",
      "121 138.85791015625\n",
      "122 131.36439514160156\n",
      "123 124.29265594482422\n",
      "124 117.61023712158203\n",
      "125 111.30480194091797\n",
      "126 105.35088348388672\n",
      "127 99.7200698852539\n",
      "128 94.40113067626953\n",
      "129 89.37084197998047\n",
      "130 84.61808776855469\n",
      "131 80.12979125976562\n",
      "132 75.88599395751953\n",
      "133 71.87083435058594\n",
      "134 68.07896423339844\n",
      "135 64.48844146728516\n",
      "136 61.09483337402344\n",
      "137 57.88519287109375\n",
      "138 54.84950637817383\n",
      "139 51.977657318115234\n",
      "140 49.25700759887695\n",
      "141 46.68434143066406\n",
      "142 44.250606536865234\n",
      "143 41.943809509277344\n",
      "144 39.76190185546875\n",
      "145 37.69754409790039\n",
      "146 35.74125289916992\n",
      "147 33.89037322998047\n",
      "148 32.13725662231445\n",
      "149 30.476428985595703\n",
      "150 28.904245376586914\n",
      "151 27.414228439331055\n",
      "152 26.002716064453125\n",
      "153 24.66705894470215\n",
      "154 23.399370193481445\n",
      "155 22.19948387145996\n",
      "156 21.06369972229004\n",
      "157 19.98537826538086\n",
      "158 18.96432876586914\n",
      "159 17.996286392211914\n",
      "160 17.077896118164062\n",
      "161 16.2083797454834\n",
      "162 15.3833589553833\n",
      "163 14.601627349853516\n",
      "164 13.86018180847168\n",
      "165 13.15737533569336\n",
      "166 12.490751266479492\n",
      "167 11.858991622924805\n",
      "168 11.259431838989258\n",
      "169 10.690351486206055\n",
      "170 10.151276588439941\n",
      "171 9.639571189880371\n",
      "172 9.153726577758789\n",
      "173 8.693364143371582\n",
      "174 8.25607681274414\n",
      "175 7.841401100158691\n",
      "176 7.448088645935059\n",
      "177 7.074516296386719\n",
      "178 6.719784259796143\n",
      "179 6.383752822875977\n",
      "180 6.064504623413086\n",
      "181 5.761423110961914\n",
      "182 5.473936080932617\n",
      "183 5.200991630554199\n",
      "184 4.941783905029297\n",
      "185 4.695966720581055\n",
      "186 4.4621405601501465\n",
      "187 4.239992618560791\n",
      "188 4.029749393463135\n",
      "189 3.8298423290252686\n",
      "190 3.63968563079834\n",
      "191 3.4593162536621094\n",
      "192 3.287933349609375\n",
      "193 3.1252691745758057\n",
      "194 2.970613956451416\n",
      "195 2.82387375831604\n",
      "196 2.6843349933624268\n",
      "197 2.5519967079162598\n",
      "198 2.4260284900665283\n",
      "199 2.306549310684204\n",
      "200 2.192960023880005\n",
      "201 2.084906578063965\n",
      "202 1.9823951721191406\n",
      "203 1.8851820230484009\n",
      "204 1.7924827337265015\n",
      "205 1.704480528831482\n",
      "206 1.6208502054214478\n",
      "207 1.5413553714752197\n",
      "208 1.4657528400421143\n",
      "209 1.3940668106079102\n",
      "210 1.325836181640625\n",
      "211 1.2610481977462769\n",
      "212 1.1994205713272095\n",
      "213 1.1407325267791748\n",
      "214 1.085126519203186\n",
      "215 1.0322519540786743\n",
      "216 0.9818016886711121\n",
      "217 0.9338846206665039\n",
      "218 0.8884141445159912\n",
      "219 0.8452199101448059\n",
      "220 0.804074764251709\n",
      "221 0.7649228572845459\n",
      "222 0.7277970314025879\n",
      "223 0.6924371123313904\n",
      "224 0.6588466167449951\n",
      "225 0.6269011497497559\n",
      "226 0.5965046882629395\n",
      "227 0.567542552947998\n",
      "228 0.5400270223617554\n",
      "229 0.5138503313064575\n",
      "230 0.4890263080596924\n",
      "231 0.4654788076877594\n",
      "232 0.4429096579551697\n",
      "233 0.42153996229171753\n",
      "234 0.40116551518440247\n",
      "235 0.38176700472831726\n",
      "236 0.36337214708328247\n",
      "237 0.3457851707935333\n",
      "238 0.3290732502937317\n",
      "239 0.3132379651069641\n",
      "240 0.2981913983821869\n",
      "241 0.2837730348110199\n",
      "242 0.27014291286468506\n",
      "243 0.2571636438369751\n",
      "244 0.24479447305202484\n",
      "245 0.23297972977161407\n",
      "246 0.22187666594982147\n",
      "247 0.2111460268497467\n",
      "248 0.20099450647830963\n",
      "249 0.19143228232860565\n",
      "250 0.1822705715894699\n",
      "251 0.17348329722881317\n",
      "252 0.16515332460403442\n",
      "253 0.15726889669895172\n",
      "254 0.14976239204406738\n",
      "255 0.14257384836673737\n",
      "256 0.1357100009918213\n",
      "257 0.12927395105361938\n",
      "258 0.12307427823543549\n",
      "259 0.11718583852052689\n",
      "260 0.11160995811223984\n",
      "261 0.10628470033407211\n",
      "262 0.10118856281042099\n",
      "263 0.09636742621660233\n",
      "264 0.09177535772323608\n",
      "265 0.087411068379879\n",
      "266 0.08323774486780167\n",
      "267 0.07927338033914566\n",
      "268 0.07549338787794113\n",
      "269 0.07191365212202072\n",
      "270 0.06847003847360611\n",
      "271 0.06521984189748764\n",
      "272 0.062131330370903015\n",
      "273 0.05916744843125343\n",
      "274 0.0563751682639122\n",
      "275 0.05371011793613434\n",
      "276 0.0511496365070343\n",
      "277 0.048714831471443176\n",
      "278 0.04640836641192436\n",
      "279 0.044221241027116776\n",
      "280 0.04212438315153122\n",
      "281 0.040130045264959335\n",
      "282 0.03825705498456955\n",
      "283 0.036430712789297104\n",
      "284 0.034707602113485336\n",
      "285 0.033064693212509155\n",
      "286 0.03150084614753723\n",
      "287 0.03002425841987133\n",
      "288 0.028608234599232674\n",
      "289 0.027264157310128212\n",
      "290 0.025978177785873413\n",
      "291 0.02475113607943058\n",
      "292 0.023612722754478455\n",
      "293 0.02250484563410282\n",
      "294 0.02143814042210579\n",
      "295 0.02043326385319233\n",
      "296 0.01948363706469536\n",
      "297 0.01858419179916382\n",
      "298 0.01771671324968338\n",
      "299 0.016888005658984184\n",
      "300 0.016094962134957314\n",
      "301 0.015349745750427246\n",
      "302 0.01464193593710661\n",
      "303 0.013965629041194916\n",
      "304 0.013319174759089947\n",
      "305 0.012692260555922985\n",
      "306 0.012108531780540943\n",
      "307 0.011553862132132053\n",
      "308 0.011018139310181141\n",
      "309 0.010508420877158642\n",
      "310 0.01002271007746458\n",
      "311 0.009563463740050793\n",
      "312 0.009125995449721813\n",
      "313 0.008710571564733982\n",
      "314 0.008311106823384762\n",
      "315 0.007935388013720512\n",
      "316 0.007571236230432987\n",
      "317 0.007227601483464241\n",
      "318 0.006900801323354244\n",
      "319 0.00658703688532114\n",
      "320 0.006295248866081238\n",
      "321 0.006013416685163975\n",
      "322 0.005738264415413141\n",
      "323 0.005487803369760513\n",
      "324 0.0052475761622190475\n",
      "325 0.005015228874981403\n",
      "326 0.004794992972165346\n",
      "327 0.004584719426929951\n",
      "328 0.004383650608360767\n",
      "329 0.0041897972114384174\n",
      "330 0.00400741770863533\n",
      "331 0.003834750736132264\n",
      "332 0.003672516206279397\n",
      "333 0.0035143555141985416\n",
      "334 0.003365728771314025\n",
      "335 0.0032191334757953882\n",
      "336 0.003080602502450347\n",
      "337 0.002951952163130045\n",
      "338 0.0028284077998250723\n",
      "339 0.002711728448048234\n",
      "340 0.0025967007968574762\n",
      "341 0.002492254599928856\n",
      "342 0.0023883823305368423\n",
      "343 0.00229451316408813\n",
      "344 0.0021998088341206312\n",
      "345 0.0021140645258128643\n",
      "346 0.0020283618941903114\n",
      "347 0.0019462541677057743\n",
      "348 0.0018702410161495209\n",
      "349 0.0017970490735024214\n",
      "350 0.0017251233803108335\n",
      "351 0.0016586383571848273\n",
      "352 0.0015945271588861942\n",
      "353 0.0015311073511838913\n",
      "354 0.0014733840944245458\n",
      "355 0.0014169582864269614\n",
      "356 0.0013646710431203246\n",
      "357 0.00131377880461514\n",
      "358 0.0012637455947697163\n",
      "359 0.0012168773682788014\n",
      "360 0.0011731513077393174\n",
      "361 0.0011306783417239785\n",
      "362 0.001088408287614584\n",
      "363 0.0010479778284206986\n",
      "364 0.0010108787100762129\n",
      "365 0.0009765616850927472\n",
      "366 0.0009414699743501842\n",
      "367 0.0009078124421648681\n",
      "368 0.0008757382747717202\n",
      "369 0.0008461720426566899\n",
      "370 0.0008171817753463984\n",
      "371 0.0007892090361565351\n",
      "372 0.0007632835768163204\n",
      "373 0.0007383024203591049\n",
      "374 0.0007139970548450947\n",
      "375 0.000690054555889219\n",
      "376 0.0006676370394416153\n",
      "377 0.0006469059153459966\n",
      "378 0.0006253802566789091\n",
      "379 0.0006063695764169097\n",
      "380 0.0005864918348379433\n",
      "381 0.0005669124657288194\n",
      "382 0.0005496941739693284\n",
      "383 0.0005334857851266861\n",
      "384 0.000516869651619345\n",
      "385 0.0005004318081773818\n",
      "386 0.0004854066064581275\n",
      "387 0.00047080282820388675\n",
      "388 0.00045680091716349125\n",
      "389 0.0004431241250131279\n",
      "390 0.00043026552884839475\n",
      "391 0.0004177363298367709\n",
      "392 0.0004056369070895016\n",
      "393 0.00039314519381150603\n",
      "394 0.0003817709512077272\n",
      "395 0.0003714864724315703\n",
      "396 0.0003609160194173455\n",
      "397 0.00035091699101030827\n",
      "398 0.00034076799056492746\n",
      "399 0.00033115182304754853\n",
      "400 0.00032250277581624687\n",
      "401 0.000313906348310411\n",
      "402 0.00030531288939528167\n",
      "403 0.0002964326413348317\n",
      "404 0.0002879236708395183\n",
      "405 0.00028131931321695447\n",
      "406 0.0002735698944889009\n",
      "407 0.00026723602786660194\n",
      "408 0.00025991257280111313\n",
      "409 0.0002533515798859298\n",
      "410 0.00024630772531963885\n",
      "411 0.00024017796386033297\n",
      "412 0.00023475832131225616\n",
      "413 0.00022838085715193301\n",
      "414 0.0002226205833721906\n",
      "415 0.0002174245601054281\n",
      "416 0.00021213230502326041\n",
      "417 0.00020631661755032837\n",
      "418 0.0002013981866184622\n",
      "419 0.00019638917001429945\n",
      "420 0.00019158009672537446\n",
      "421 0.00018716949853114784\n",
      "422 0.0001829454704420641\n",
      "423 0.00017819626373238862\n",
      "424 0.00017438903159927577\n",
      "425 0.00017038066289387643\n",
      "426 0.00016589288134127855\n",
      "427 0.00016246072482317686\n",
      "428 0.00015844010340515524\n",
      "429 0.00015505975170526654\n",
      "430 0.0001517580822110176\n",
      "431 0.00014827844279352576\n",
      "432 0.00014543533325195312\n",
      "433 0.00014246979844756424\n",
      "434 0.0001390064280712977\n",
      "435 0.0001358176814392209\n",
      "436 0.00013297410623636097\n",
      "437 0.000130432250443846\n",
      "438 0.00012776906078215688\n",
      "439 0.0001251231151400134\n",
      "440 0.00012244113895576447\n",
      "441 0.00011973540676990524\n",
      "442 0.00011700634058797732\n",
      "443 0.00011464324779808521\n",
      "444 0.00011218508007004857\n",
      "445 0.00011030304449377581\n",
      "446 0.00010788554936880246\n",
      "447 0.00010563869727775455\n",
      "448 0.00010353280231356621\n",
      "449 0.00010189563181484118\n",
      "450 9.976608271244913e-05\n",
      "451 9.786739974515513e-05\n",
      "452 9.623814548831433e-05\n",
      "453 9.454692917643115e-05\n",
      "454 9.265432163374498e-05\n",
      "455 9.053372195921838e-05\n",
      "456 8.907973096938804e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457 8.734289440326393e-05\n",
      "458 8.566450560465455e-05\n",
      "459 8.427090506302193e-05\n",
      "460 8.272210834547877e-05\n",
      "461 8.111347415251657e-05\n",
      "462 7.957714115036651e-05\n",
      "463 7.83625291660428e-05\n",
      "464 7.690851634833962e-05\n",
      "465 7.559984805993736e-05\n",
      "466 7.458833715645596e-05\n",
      "467 7.326616469072178e-05\n",
      "468 7.177137740654871e-05\n",
      "469 7.059671042952687e-05\n",
      "470 6.940193998161703e-05\n",
      "471 6.821868009865284e-05\n",
      "472 6.724429113091901e-05\n",
      "473 6.611106800846756e-05\n",
      "474 6.49686116958037e-05\n",
      "475 6.39939826214686e-05\n",
      "476 6.307630974333733e-05\n",
      "477 6.17390323895961e-05\n",
      "478 6.063824548618868e-05\n",
      "479 5.9796220739372075e-05\n",
      "480 5.865429193363525e-05\n",
      "481 5.811507435282692e-05\n",
      "482 5.7252011174568906e-05\n",
      "483 5.6421304179821163e-05\n",
      "484 5.560977660934441e-05\n",
      "485 5.4571359214605764e-05\n",
      "486 5.38077401870396e-05\n",
      "487 5.265368235995993e-05\n",
      "488 5.1889001042582095e-05\n",
      "489 5.1280501793371513e-05\n",
      "490 5.050922482041642e-05\n",
      "491 4.97412183904089e-05\n",
      "492 4.8996229452313855e-05\n",
      "493 4.820764297619462e-05\n",
      "494 4.75244851259049e-05\n",
      "495 4.670946509577334e-05\n",
      "496 4.5929384214105085e-05\n",
      "497 4.534826075541787e-05\n",
      "498 4.4745782361133024e-05\n",
      "499 4.403726416057907e-05\n"
     ]
    }
   ],
   "source": [
    "#Tensor와 autograd를 사용한 신경망(ppt 14p)\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=False로 설정하여 역전파 중에 이 Tensor들에 대한 변화도를 계산할\n",
    "# 필요가 없음을 나타냅니다. (requres_grad의 기본값이 False이므로 아래 코드에는\n",
    "# 이를 반영하지 않았습니다.)\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=True로 설정하여 역전파 중에 이 Tensor들에 대한\n",
    "# 변화도를 계산할 필요가 있음을 나타냅니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다. 이는 Tensor를\n",
    "    # 사용한 순전파 단계와 완전히 동일하지만, 역전파 단계를 별도로 구현하지 않아도\n",
    "    # 되므로 중간값들에 대한 참조(reference)를 갖고 있을 필요가 없습니다.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Tensor 연산을 사용하여 손실을 계산하고 출력합니다.\n",
    "    # loss는 (1,) 형태의 Tensor이며, loss.item()은 loss의 스칼라 값입니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # autograd를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를\n",
    "    # 갖는 모든 Tensor에 대해 손실의 변화도를 계산합니다. 이후 w1.grad와 w2.grad는\n",
    "    # w1과 w2 각각에 대한 손실의 변화도를 갖는 Tensor가 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 수동으로 갱신합니다.\n",
    "    # torch.no_grad()로 감싸는 이유는 가중치들이 requires_grad=True이지만\n",
    "    # autograd에서는 이를 추적할 필요가 없기 때문입니다.\n",
    "    # 다른 방법은 weight.data 및 weight.grad.data를 조작하는 방법입니다.\n",
    "    # tensor.data가 tensor의 저장공간을 공유하기는 하지만, 이력을\n",
    "    # 추적하지 않는다는 것을 기억하십시오.\n",
    "    # 또한, 이를 위해 torch.optim.SGD 를 사용할 수도 있습니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30230070.0\n",
      "1 25202984.0\n",
      "2 25747116.0\n",
      "3 27733998.0\n",
      "4 27627206.0\n",
      "5 23784120.0\n",
      "6 16894426.0\n",
      "7 10292329.0\n",
      "8 5696042.0\n",
      "9 3164103.0\n",
      "10 1880654.875\n",
      "11 1240632.875\n",
      "12 901462.6875\n",
      "13 703951.125\n",
      "14 575569.9375\n",
      "15 483809.4375\n",
      "16 413566.9375\n",
      "17 357229.0625\n",
      "18 310743.3125\n",
      "19 271684.65625\n",
      "20 238528.4375\n",
      "21 210159.5625\n",
      "22 185730.5625\n",
      "23 164609.28125\n",
      "24 146268.3125\n",
      "25 130276.921875\n",
      "26 116298.140625\n",
      "27 104032.1875\n",
      "28 93235.90625\n",
      "29 83715.3984375\n",
      "30 75310.3046875\n",
      "31 67866.890625\n",
      "32 61260.25390625\n",
      "33 55381.30859375\n",
      "34 50142.3359375\n",
      "35 45463.828125\n",
      "36 41275.703125\n",
      "37 37520.50390625\n",
      "38 34147.953125\n",
      "39 31117.255859375\n",
      "40 28387.71875\n",
      "41 25925.9296875\n",
      "42 23702.892578125\n",
      "43 21690.8984375\n",
      "44 19867.87890625\n",
      "45 18214.69921875\n",
      "46 16712.794921875\n",
      "47 15342.85546875\n",
      "48 14099.51953125\n",
      "49 12967.5107421875\n",
      "50 11935.6416015625\n",
      "51 10994.1826171875\n",
      "52 10134.0390625\n",
      "53 9347.9921875\n",
      "54 8628.5107421875\n",
      "55 7969.716796875\n",
      "56 7365.62060546875\n",
      "57 6811.48876953125\n",
      "58 6302.64794921875\n",
      "59 5835.037109375\n",
      "60 5405.25048828125\n",
      "61 5009.9716796875\n",
      "62 4645.9013671875\n",
      "63 4310.5400390625\n",
      "64 4001.329833984375\n",
      "65 3716.174560546875\n",
      "66 3452.934814453125\n",
      "67 3209.77099609375\n",
      "68 2985.127685546875\n",
      "69 2777.403076171875\n",
      "70 2584.915283203125\n",
      "71 2406.741455078125\n",
      "72 2241.785888671875\n",
      "73 2089.012451171875\n",
      "74 1947.399658203125\n",
      "75 1816.078857421875\n",
      "76 1694.253173828125\n",
      "77 1581.205322265625\n",
      "78 1476.27197265625\n",
      "79 1378.7442626953125\n",
      "80 1288.1007080078125\n",
      "81 1203.7965087890625\n",
      "82 1125.3939208984375\n",
      "83 1052.4561767578125\n",
      "84 984.5463256835938\n",
      "85 921.3046875\n",
      "86 862.4060668945312\n",
      "87 807.533935546875\n",
      "88 756.3596801757812\n",
      "89 708.6502075195312\n",
      "90 664.1588134765625\n",
      "91 622.6510009765625\n",
      "92 583.9017333984375\n",
      "93 547.73291015625\n",
      "94 513.951171875\n",
      "95 482.3721923828125\n",
      "96 452.855224609375\n",
      "97 425.25189208984375\n",
      "98 399.4405212402344\n",
      "99 375.29315185546875\n",
      "100 352.6847839355469\n",
      "101 331.51953125\n",
      "102 311.7013244628906\n",
      "103 293.14227294921875\n",
      "104 275.7520446777344\n",
      "105 259.4570007324219\n",
      "106 244.18124389648438\n",
      "107 229.85842895507812\n",
      "108 216.42758178710938\n",
      "109 203.82907104492188\n",
      "110 192.0034942626953\n",
      "111 180.90211486816406\n",
      "112 170.47914123535156\n",
      "113 160.69102478027344\n",
      "114 151.49781799316406\n",
      "115 142.86224365234375\n",
      "116 134.7618865966797\n",
      "117 127.15575408935547\n",
      "118 119.99966430664062\n",
      "119 113.27137756347656\n",
      "120 106.94120025634766\n",
      "121 100.98535919189453\n",
      "122 95.37962341308594\n",
      "123 90.1006851196289\n",
      "124 85.12882232666016\n",
      "125 80.44973754882812\n",
      "126 76.0417251586914\n",
      "127 71.88935852050781\n",
      "128 67.9743881225586\n",
      "129 64.28407287597656\n",
      "130 60.80501174926758\n",
      "131 57.523956298828125\n",
      "132 54.431888580322266\n",
      "133 51.513160705566406\n",
      "134 48.75895690917969\n",
      "135 46.1606330871582\n",
      "136 43.70844650268555\n",
      "137 41.39115524291992\n",
      "138 39.20454406738281\n",
      "139 37.139625549316406\n",
      "140 35.187992095947266\n",
      "141 33.34476852416992\n",
      "142 31.604095458984375\n",
      "143 29.958538055419922\n",
      "144 28.402740478515625\n",
      "145 26.931520462036133\n",
      "146 25.540849685668945\n",
      "147 24.22515106201172\n",
      "148 22.981563568115234\n",
      "149 21.802989959716797\n",
      "150 20.688339233398438\n",
      "151 19.6344051361084\n",
      "152 18.63654327392578\n",
      "153 17.691741943359375\n",
      "154 16.79636001586914\n",
      "155 15.949368476867676\n",
      "156 15.146316528320312\n",
      "157 14.38598918914795\n",
      "158 13.665450096130371\n",
      "159 12.982626914978027\n",
      "160 12.336287498474121\n",
      "161 11.722747802734375\n",
      "162 11.1412935256958\n",
      "163 10.589938163757324\n",
      "164 10.067097663879395\n",
      "165 9.571287155151367\n",
      "166 9.10076904296875\n",
      "167 8.654496192932129\n",
      "168 8.231219291687012\n",
      "169 7.830018997192383\n",
      "170 7.448638916015625\n",
      "171 7.08650541305542\n",
      "172 6.742861747741699\n",
      "173 6.416233062744141\n",
      "174 6.1065263748168945\n",
      "175 5.812200546264648\n",
      "176 5.532839775085449\n",
      "177 5.26771879196167\n",
      "178 5.015223503112793\n",
      "179 4.775647163391113\n",
      "180 4.547907829284668\n",
      "181 4.331287860870361\n",
      "182 4.1256794929504395\n",
      "183 3.929961919784546\n",
      "184 3.7439517974853516\n",
      "185 3.566951036453247\n",
      "186 3.399015188217163\n",
      "187 3.2389450073242188\n",
      "188 3.0869762897491455\n",
      "189 2.94242262840271\n",
      "190 2.8045177459716797\n",
      "191 2.6733648777008057\n",
      "192 2.54884672164917\n",
      "193 2.4303488731384277\n",
      "194 2.317282199859619\n",
      "195 2.2096850872039795\n",
      "196 2.1074941158294678\n",
      "197 2.0100691318511963\n",
      "198 1.9173306226730347\n",
      "199 1.8289525508880615\n",
      "200 1.744788408279419\n",
      "201 1.6646772623062134\n",
      "202 1.5882431268692017\n",
      "203 1.5156652927398682\n",
      "204 1.4465267658233643\n",
      "205 1.3804551362991333\n",
      "206 1.3175363540649414\n",
      "207 1.2575159072875977\n",
      "208 1.2003663778305054\n",
      "209 1.1460120677947998\n",
      "210 1.0940487384796143\n",
      "211 1.0446330308914185\n",
      "212 0.9973995089530945\n",
      "213 0.9524213075637817\n",
      "214 0.909575343132019\n",
      "215 0.8686463832855225\n",
      "216 0.8296505212783813\n",
      "217 0.7924897074699402\n",
      "218 0.7569339275360107\n",
      "219 0.7231521606445312\n",
      "220 0.6909098029136658\n",
      "221 0.6600780487060547\n",
      "222 0.6306607127189636\n",
      "223 0.602462112903595\n",
      "224 0.5758010149002075\n",
      "225 0.5502891540527344\n",
      "226 0.5258535146713257\n",
      "227 0.5025765895843506\n",
      "228 0.4803204834461212\n",
      "229 0.45917612314224243\n",
      "230 0.4389040470123291\n",
      "231 0.41957882046699524\n",
      "232 0.40107786655426025\n",
      "233 0.3834242522716522\n",
      "234 0.36662420630455017\n",
      "235 0.35057103633880615\n",
      "236 0.3351888358592987\n",
      "237 0.32054248452186584\n",
      "238 0.3065108060836792\n",
      "239 0.2931433618068695\n",
      "240 0.28035128116607666\n",
      "241 0.2681313753128052\n",
      "242 0.25645148754119873\n",
      "243 0.2453290820121765\n",
      "244 0.23467381298542023\n",
      "245 0.22447605431079865\n",
      "246 0.2147556096315384\n",
      "247 0.20543630421161652\n",
      "248 0.1965530514717102\n",
      "249 0.18801377713680267\n",
      "250 0.17992226779460907\n",
      "251 0.17216360569000244\n",
      "252 0.16474515199661255\n",
      "253 0.15764597058296204\n",
      "254 0.15087082982063293\n",
      "255 0.14438802003860474\n",
      "256 0.1382504552602768\n",
      "257 0.13230004906654358\n",
      "258 0.12662556767463684\n",
      "259 0.12118357419967651\n",
      "260 0.11602224409580231\n",
      "261 0.11102738231420517\n",
      "262 0.10627683997154236\n",
      "263 0.10172905027866364\n",
      "264 0.09739238023757935\n",
      "265 0.09325119107961655\n",
      "266 0.08928079158067703\n",
      "267 0.08551057428121567\n",
      "268 0.08188161998987198\n",
      "269 0.07841382920742035\n",
      "270 0.07509376108646393\n",
      "271 0.07189524173736572\n",
      "272 0.06886269152164459\n",
      "273 0.06595411896705627\n",
      "274 0.06318977475166321\n",
      "275 0.060481954365968704\n",
      "276 0.057934507727622986\n",
      "277 0.05551191419363022\n",
      "278 0.05317261070013046\n",
      "279 0.05093011632561684\n",
      "280 0.048783741891384125\n",
      "281 0.04673989117145538\n",
      "282 0.04479003697633743\n",
      "283 0.042898327112197876\n",
      "284 0.04110847786068916\n",
      "285 0.0393856056034565\n",
      "286 0.0377567820250988\n",
      "287 0.03617244213819504\n",
      "288 0.03466011583805084\n",
      "289 0.03322537615895271\n",
      "290 0.03183165565133095\n",
      "291 0.03051462583243847\n",
      "292 0.02923116646707058\n",
      "293 0.02803703583776951\n",
      "294 0.026877254247665405\n",
      "295 0.025767052546143532\n",
      "296 0.02469545602798462\n",
      "297 0.02367841824889183\n",
      "298 0.022695712745189667\n",
      "299 0.021752387285232544\n",
      "300 0.020858777686953545\n",
      "301 0.019997041672468185\n",
      "302 0.01918310672044754\n",
      "303 0.018393000587821007\n",
      "304 0.017622750252485275\n",
      "305 0.016922837123274803\n",
      "306 0.01622743345797062\n",
      "307 0.015573445707559586\n",
      "308 0.014928251504898071\n",
      "309 0.01431879960000515\n",
      "310 0.013740302994847298\n",
      "311 0.013184940442442894\n",
      "312 0.012645583599805832\n",
      "313 0.012141323648393154\n",
      "314 0.011657297611236572\n",
      "315 0.011181297712028027\n",
      "316 0.010734171606600285\n",
      "317 0.010300550609827042\n",
      "318 0.009885028004646301\n",
      "319 0.009489564225077629\n",
      "320 0.009116609580814838\n",
      "321 0.008754000999033451\n",
      "322 0.008406338281929493\n",
      "323 0.008071120828390121\n",
      "324 0.0077444929629564285\n",
      "325 0.007446620147675276\n",
      "326 0.007145238108932972\n",
      "327 0.006869804114103317\n",
      "328 0.006599409505724907\n",
      "329 0.006347570102661848\n",
      "330 0.006096904631704092\n",
      "331 0.005861115176230669\n",
      "332 0.00563475489616394\n",
      "333 0.005420822184532881\n",
      "334 0.0052078901790082455\n",
      "335 0.00501195527613163\n",
      "336 0.004817941226065159\n",
      "337 0.004638988059014082\n",
      "338 0.004461184144020081\n",
      "339 0.004290602635592222\n",
      "340 0.004132539499551058\n",
      "341 0.003977512940764427\n",
      "342 0.0038289830554276705\n",
      "343 0.0036858271341770887\n",
      "344 0.003549081739038229\n",
      "345 0.003419587155804038\n",
      "346 0.0032927743159234524\n",
      "347 0.0031686341390013695\n",
      "348 0.003055908251553774\n",
      "349 0.002944291103631258\n",
      "350 0.0028384439647197723\n",
      "351 0.0027360208332538605\n",
      "352 0.0026385867968201637\n",
      "353 0.0025401650927960873\n",
      "354 0.0024521243758499622\n",
      "355 0.002364466432482004\n",
      "356 0.002282713307067752\n",
      "357 0.0021962292958050966\n",
      "358 0.0021224692463874817\n",
      "359 0.002047299174591899\n",
      "360 0.001977212494239211\n",
      "361 0.001911198953166604\n",
      "362 0.0018458807608112693\n",
      "363 0.0017811275320127606\n",
      "364 0.0017212590901181102\n",
      "365 0.0016627954319119453\n",
      "366 0.0016077280743047595\n",
      "367 0.0015527138020843267\n",
      "368 0.0015015427488833666\n",
      "369 0.0014534629881381989\n",
      "370 0.001404315116815269\n",
      "371 0.0013597824145108461\n",
      "372 0.001315517001785338\n",
      "373 0.0012735470663756132\n",
      "374 0.0012336554937064648\n",
      "375 0.0011933159548789263\n",
      "376 0.001155439647845924\n",
      "377 0.0011190633522346616\n",
      "378 0.001084125367924571\n",
      "379 0.0010510551510378718\n",
      "380 0.001017569680698216\n",
      "381 0.0009865512838587165\n",
      "382 0.000954325427301228\n",
      "383 0.0009261202649213374\n",
      "384 0.0008999790297821164\n",
      "385 0.0008728656102903187\n",
      "386 0.000847887946292758\n",
      "387 0.0008225910714827478\n",
      "388 0.0007977965287864208\n",
      "389 0.000775086518842727\n",
      "390 0.0007516838959418237\n",
      "391 0.000730998523067683\n",
      "392 0.000710195628926158\n",
      "393 0.0006907562492415309\n",
      "394 0.0006717395735904574\n",
      "395 0.00065271818311885\n",
      "396 0.0006343930726870894\n",
      "397 0.0006171232089400291\n",
      "398 0.0005998252891004086\n",
      "399 0.0005833907634951174\n",
      "400 0.0005671036196872592\n",
      "401 0.0005519675905816257\n",
      "402 0.0005368530401028693\n",
      "403 0.0005222970503382385\n",
      "404 0.0005084646982140839\n",
      "405 0.0004954588366672397\n",
      "406 0.0004822318151127547\n",
      "407 0.0004698033444583416\n",
      "408 0.0004583706322591752\n",
      "409 0.0004465508973225951\n",
      "410 0.00043393761734478176\n",
      "411 0.00042391952592879534\n",
      "412 0.0004127324791625142\n",
      "413 0.0004016229650005698\n",
      "414 0.00039147146162576973\n",
      "415 0.0003820503188762814\n",
      "416 0.0003725973074324429\n",
      "417 0.0003642510564532131\n",
      "418 0.00035562526318244636\n",
      "419 0.0003471848904155195\n",
      "420 0.0003385384916327894\n",
      "421 0.00033084419555962086\n",
      "422 0.00032311008544638753\n",
      "423 0.0003162582579534501\n",
      "424 0.0003083061892539263\n",
      "425 0.0003017642302438617\n",
      "426 0.0002942355931736529\n",
      "427 0.0002876617945730686\n",
      "428 0.00028193843900226057\n",
      "429 0.00027556094573810697\n",
      "430 0.00026905996492132545\n",
      "431 0.0002635876589920372\n",
      "432 0.0002570684882812202\n",
      "433 0.00025171320885419846\n",
      "434 0.0002467975136823952\n",
      "435 0.0002413816546322778\n",
      "436 0.00023620622232556343\n",
      "437 0.0002313531149411574\n",
      "438 0.00022672662453260273\n",
      "439 0.00022232244373299181\n",
      "440 0.00021741425734944642\n",
      "441 0.00021260319044813514\n",
      "442 0.00020860848599113524\n",
      "443 0.00020405801478773355\n",
      "444 0.00020019107614643872\n",
      "445 0.00019672584312502295\n",
      "446 0.00019303068984299898\n",
      "447 0.00018897086556535214\n",
      "448 0.0001849714753916487\n",
      "449 0.0001817056763684377\n",
      "450 0.00017858626961242408\n",
      "451 0.00017458490037824959\n",
      "452 0.0001714742393232882\n",
      "453 0.00016833377594593912\n",
      "454 0.00016532110748812556\n",
      "455 0.0001620141847524792\n",
      "456 0.00015911631635390222\n",
      "457 0.0001569408195791766\n",
      "458 0.00015371048357337713\n",
      "459 0.00015030091162770987\n",
      "460 0.0001476993056712672\n",
      "461 0.0001452016003895551\n",
      "462 0.0001423915382474661\n",
      "463 0.00014012091560289264\n",
      "464 0.0001378495362587273\n",
      "465 0.00013570135342888534\n",
      "466 0.00013388314982876182\n",
      "467 0.00013149302685633302\n",
      "468 0.0001295588444918394\n",
      "469 0.0001267678599106148\n",
      "470 0.00012496489216573536\n",
      "471 0.00012283827527426183\n",
      "472 0.00012063836766174063\n",
      "473 0.00011844853725051507\n",
      "474 0.00011665774218272418\n",
      "475 0.00011470958997961134\n",
      "476 0.00011286889639450237\n",
      "477 0.00011085237929364666\n",
      "478 0.00010921474313363433\n",
      "479 0.00010743152961367741\n",
      "480 0.00010599611414363608\n",
      "481 0.00010418022429803386\n",
      "482 0.00010274477244820446\n",
      "483 0.00010132557508768514\n",
      "484 0.00010021854541264474\n",
      "485 9.877863340079784e-05\n",
      "486 9.702294482849538e-05\n",
      "487 9.551671973895282e-05\n",
      "488 9.404576849192381e-05\n",
      "489 9.260907245334238e-05\n",
      "490 9.133105049841106e-05\n",
      "491 9.014113311422989e-05\n",
      "492 8.87129208422266e-05\n",
      "493 8.740735938772559e-05\n",
      "494 8.594787504989654e-05\n",
      "495 8.496664668200538e-05\n",
      "496 8.383848762605339e-05\n",
      "497 8.28341071610339e-05\n",
      "498 8.136904216371477e-05\n",
      "499 8.067889575613663e-05\n"
     ]
    }
   ],
   "source": [
    "#ReLU로 비선형적으로 동작하는 사용자 정의 autograd(ppt 15p)\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 682.2491455078125\n",
      "1 634.35205078125\n",
      "2 592.9074096679688\n",
      "3 556.3314819335938\n",
      "4 523.4736938476562\n",
      "5 493.4969482421875\n",
      "6 466.0244445800781\n",
      "7 440.6562805175781\n",
      "8 417.12353515625\n",
      "9 395.0384826660156\n",
      "10 374.197021484375\n",
      "11 354.5400390625\n",
      "12 335.9761657714844\n",
      "13 318.3549499511719\n",
      "14 301.5140380859375\n",
      "15 285.46392822265625\n",
      "16 270.1981201171875\n",
      "17 255.64572143554688\n",
      "18 241.8213653564453\n",
      "19 228.6401824951172\n",
      "20 216.02700805664062\n",
      "21 203.96524047851562\n",
      "22 192.43975830078125\n",
      "23 181.46685791015625\n",
      "24 171.04054260253906\n",
      "25 161.13113403320312\n",
      "26 151.72518920898438\n",
      "27 142.778564453125\n",
      "28 134.2759246826172\n",
      "29 126.2290267944336\n",
      "30 118.61138916015625\n",
      "31 111.4321517944336\n",
      "32 104.63601684570312\n",
      "33 98.22862243652344\n",
      "34 92.18360137939453\n",
      "35 86.51065826416016\n",
      "36 81.16575622558594\n",
      "37 76.15743255615234\n",
      "38 71.45620727539062\n",
      "39 67.04847717285156\n",
      "40 62.90892791748047\n",
      "41 59.02996063232422\n",
      "42 55.39828872680664\n",
      "43 51.99351119995117\n",
      "44 48.80507278442383\n",
      "45 45.82244873046875\n",
      "46 43.029640197753906\n",
      "47 40.413047790527344\n",
      "48 37.96566390991211\n",
      "49 35.676239013671875\n",
      "50 33.53020095825195\n",
      "51 31.518367767333984\n",
      "52 29.640119552612305\n",
      "53 27.882627487182617\n",
      "54 26.23458480834961\n",
      "55 24.692304611206055\n",
      "56 23.247764587402344\n",
      "57 21.894838333129883\n",
      "58 20.627580642700195\n",
      "59 19.441213607788086\n",
      "60 18.32962989807129\n",
      "61 17.286848068237305\n",
      "62 16.310728073120117\n",
      "63 15.394857406616211\n",
      "64 14.53582763671875\n",
      "65 13.728418350219727\n",
      "66 12.970111846923828\n",
      "67 12.259812355041504\n",
      "68 11.5905179977417\n",
      "69 10.962174415588379\n",
      "70 10.371153831481934\n",
      "71 9.815878868103027\n",
      "72 9.292820930480957\n",
      "73 8.801178932189941\n",
      "74 8.337928771972656\n",
      "75 7.901446342468262\n",
      "76 7.489513397216797\n",
      "77 7.101409435272217\n",
      "78 6.735388278961182\n",
      "79 6.389980316162109\n",
      "80 6.064246654510498\n",
      "81 5.756988525390625\n",
      "82 5.467133045196533\n",
      "83 5.192838668823242\n",
      "84 4.933292388916016\n",
      "85 4.687259674072266\n",
      "86 4.4540510177612305\n",
      "87 4.233087062835693\n",
      "88 4.024326801300049\n",
      "89 3.826846122741699\n",
      "90 3.6402297019958496\n",
      "91 3.4641032218933105\n",
      "92 3.2972211837768555\n",
      "93 3.1391024589538574\n",
      "94 2.9893014430999756\n",
      "95 2.847468137741089\n",
      "96 2.7130279541015625\n",
      "97 2.5855021476745605\n",
      "98 2.4646294116973877\n",
      "99 2.3498616218566895\n",
      "100 2.2409310340881348\n",
      "101 2.1372880935668945\n",
      "102 2.038895606994629\n",
      "103 1.9453580379486084\n",
      "104 1.856681227684021\n",
      "105 1.7723665237426758\n",
      "106 1.6920446157455444\n",
      "107 1.6155823469161987\n",
      "108 1.5429227352142334\n",
      "109 1.4739248752593994\n",
      "110 1.408125400543213\n",
      "111 1.3455098867416382\n",
      "112 1.2859113216400146\n",
      "113 1.2292636632919312\n",
      "114 1.1753730773925781\n",
      "115 1.1240595579147339\n",
      "116 1.075162649154663\n",
      "117 1.0284844636917114\n",
      "118 0.9840359687805176\n",
      "119 0.9416446685791016\n",
      "120 0.901157021522522\n",
      "121 0.8625887036323547\n",
      "122 0.8258161544799805\n",
      "123 0.7907565236091614\n",
      "124 0.7573472261428833\n",
      "125 0.7254774570465088\n",
      "126 0.6950190663337708\n",
      "127 0.6659342646598816\n",
      "128 0.6381991505622864\n",
      "129 0.6116700768470764\n",
      "130 0.5863009095191956\n",
      "131 0.5621183514595032\n",
      "132 0.5390597581863403\n",
      "133 0.516995906829834\n",
      "134 0.4959002733230591\n",
      "135 0.47569143772125244\n",
      "136 0.4563644528388977\n",
      "137 0.4378873109817505\n",
      "138 0.4202058017253876\n",
      "139 0.40327709913253784\n",
      "140 0.3870854377746582\n",
      "141 0.37158769369125366\n",
      "142 0.3567466735839844\n",
      "143 0.3425396680831909\n",
      "144 0.32893332839012146\n",
      "145 0.315896213054657\n",
      "146 0.3034262955188751\n",
      "147 0.2914571762084961\n",
      "148 0.28000402450561523\n",
      "149 0.26899635791778564\n",
      "150 0.2584605813026428\n",
      "151 0.24835878610610962\n",
      "152 0.2386612892150879\n",
      "153 0.22936226427555084\n",
      "154 0.2204647660255432\n",
      "155 0.21191735565662384\n",
      "156 0.20372183620929718\n",
      "157 0.19587163627147675\n",
      "158 0.1883324533700943\n",
      "159 0.18111516535282135\n",
      "160 0.17417535185813904\n",
      "161 0.16751301288604736\n",
      "162 0.16111373901367188\n",
      "163 0.15497757494449615\n",
      "164 0.1490824818611145\n",
      "165 0.14343290030956268\n",
      "166 0.13800275325775146\n",
      "167 0.1327972412109375\n",
      "168 0.1277879923582077\n",
      "169 0.12297124415636063\n",
      "170 0.11834447830915451\n",
      "171 0.113902747631073\n",
      "172 0.10963431745767593\n",
      "173 0.10552974790334702\n",
      "174 0.10158470273017883\n",
      "175 0.09779228270053864\n",
      "176 0.09414882212877274\n",
      "177 0.0906471461057663\n",
      "178 0.08728713542222977\n",
      "179 0.08406193554401398\n",
      "180 0.08095791190862656\n",
      "181 0.07797420024871826\n",
      "182 0.07510115206241608\n",
      "183 0.07233926653862\n",
      "184 0.0696830227971077\n",
      "185 0.06713418662548065\n",
      "186 0.0646836906671524\n",
      "187 0.06232191249728203\n",
      "188 0.06004593148827553\n",
      "189 0.05785960704088211\n",
      "190 0.05575265362858772\n",
      "191 0.05372535437345505\n",
      "192 0.051774051040410995\n",
      "193 0.049898214638233185\n",
      "194 0.04809103161096573\n",
      "195 0.046353671699762344\n",
      "196 0.04467995464801788\n",
      "197 0.043068937957286835\n",
      "198 0.04151765629649162\n",
      "199 0.04002765193581581\n",
      "200 0.0385906882584095\n",
      "201 0.03720574826002121\n",
      "202 0.03587266430258751\n",
      "203 0.03458927944302559\n",
      "204 0.03335334733128548\n",
      "205 0.032163094729185104\n",
      "206 0.031018920242786407\n",
      "207 0.02991391345858574\n",
      "208 0.028848618268966675\n",
      "209 0.02782275341451168\n",
      "210 0.026834962889552116\n",
      "211 0.025883572176098824\n",
      "212 0.024967404082417488\n",
      "213 0.02408471144735813\n",
      "214 0.02323375642299652\n",
      "215 0.022413192316889763\n",
      "216 0.021623026579618454\n",
      "217 0.020862309262156487\n",
      "218 0.020128384232521057\n",
      "219 0.01942051388323307\n",
      "220 0.01873861253261566\n",
      "221 0.018080994486808777\n",
      "222 0.017447197809815407\n",
      "223 0.016835547983646393\n",
      "224 0.01624624989926815\n",
      "225 0.01567917689681053\n",
      "226 0.015132260508835316\n",
      "227 0.014603816904127598\n",
      "228 0.01409417949616909\n",
      "229 0.01360275223851204\n",
      "230 0.013129271566867828\n",
      "231 0.012673139572143555\n",
      "232 0.012232589535415173\n",
      "233 0.01180766336619854\n",
      "234 0.011397764086723328\n",
      "235 0.01100283581763506\n",
      "236 0.01062149927020073\n",
      "237 0.01025365013629198\n",
      "238 0.009898894466459751\n",
      "239 0.009557293727993965\n",
      "240 0.009227459318935871\n",
      "241 0.008908754214644432\n",
      "242 0.008601657114923\n",
      "243 0.008304845541715622\n",
      "244 0.00801850762218237\n",
      "245 0.007743550464510918\n",
      "246 0.007477182894945145\n",
      "247 0.0072198715060949326\n",
      "248 0.006971698720008135\n",
      "249 0.006732226815074682\n",
      "250 0.006501280702650547\n",
      "251 0.0062782191671431065\n",
      "252 0.0060630193911492825\n",
      "253 0.005857475101947784\n",
      "254 0.005659114103764296\n",
      "255 0.005467519164085388\n",
      "256 0.005282454192638397\n",
      "257 0.0051039233803749084\n",
      "258 0.004931767471134663\n",
      "259 0.004765552934259176\n",
      "260 0.004604852292686701\n",
      "261 0.004449683241546154\n",
      "262 0.004299947060644627\n",
      "263 0.004155254457145929\n",
      "264 0.004015965387225151\n",
      "265 0.0038814821746200323\n",
      "266 0.003751221811398864\n",
      "267 0.003625366138294339\n",
      "268 0.0035042748786509037\n",
      "269 0.0033869536127895117\n",
      "270 0.0032736784778535366\n",
      "271 0.003164232475683093\n",
      "272 0.0030586381908506155\n",
      "273 0.0029565051663666964\n",
      "274 0.002857829909771681\n",
      "275 0.002762539079412818\n",
      "276 0.002670565852895379\n",
      "277 0.0025816629640758038\n",
      "278 0.0024957102723419666\n",
      "279 0.002412691479548812\n",
      "280 0.002332444768399\n",
      "281 0.0022549291606992483\n",
      "282 0.0021801074035465717\n",
      "283 0.002107835840433836\n",
      "284 0.002037954516708851\n",
      "285 0.001970699056982994\n",
      "286 0.0019054347649216652\n",
      "287 0.0018423261353746057\n",
      "288 0.0017814331222325563\n",
      "289 0.0017226007767021656\n",
      "290 0.0016657569212839007\n",
      "291 0.0016107280971482396\n",
      "292 0.0015575382858514786\n",
      "293 0.0015061767771840096\n",
      "294 0.0014565137680619955\n",
      "295 0.0014085082802921534\n",
      "296 0.0013621327234432101\n",
      "297 0.0013174049090594053\n",
      "298 0.0012740765232592821\n",
      "299 0.0012321749236434698\n",
      "300 0.0011916597140952945\n",
      "301 0.0011525019071996212\n",
      "302 0.0011146615725010633\n",
      "303 0.0010781047167256474\n",
      "304 0.0010427491506561637\n",
      "305 0.0010085487738251686\n",
      "306 0.0009756260551512241\n",
      "307 0.0009437407134100795\n",
      "308 0.0009128455421887338\n",
      "309 0.000882999156601727\n",
      "310 0.0008541501010768116\n",
      "311 0.0008262170013040304\n",
      "312 0.0007992341415956616\n",
      "313 0.0007731268415227532\n",
      "314 0.0007478793268091977\n",
      "315 0.000723455916158855\n",
      "316 0.000699880882166326\n",
      "317 0.0006770926993340254\n",
      "318 0.0006550237303599715\n",
      "319 0.0006336875376291573\n",
      "320 0.0006130624678917229\n",
      "321 0.0005931128980591893\n",
      "322 0.0005738122272305191\n",
      "323 0.0005551568465307355\n",
      "324 0.0005371446604840457\n",
      "325 0.0005197007558308542\n",
      "326 0.0005028336308896542\n",
      "327 0.0004865257069468498\n",
      "328 0.0004708335327450186\n",
      "329 0.00045557317207567394\n",
      "330 0.00044080131920054555\n",
      "331 0.00042652495903894305\n",
      "332 0.00041271073860116303\n",
      "333 0.000399345182813704\n",
      "334 0.0003864206955768168\n",
      "335 0.0003739108797162771\n",
      "336 0.0003618242044467479\n",
      "337 0.00035012513399124146\n",
      "338 0.0003388047043699771\n",
      "339 0.00032785392249934375\n",
      "340 0.0003172677825205028\n",
      "341 0.0003070358943659812\n",
      "342 0.0002971429203171283\n",
      "343 0.0002875632490031421\n",
      "344 0.0002782962692435831\n",
      "345 0.0002693226851988584\n",
      "346 0.0002606655762065202\n",
      "347 0.00025227211881428957\n",
      "348 0.0002441520045977086\n",
      "349 0.00023629478528164327\n",
      "350 0.00022871035616844893\n",
      "351 0.0002213668340118602\n",
      "352 0.00021425471641123295\n",
      "353 0.0002073709765681997\n",
      "354 0.00020070644677616656\n",
      "355 0.00019426160724833608\n",
      "356 0.00018802154227159917\n",
      "357 0.0001819922326831147\n",
      "358 0.00017615426622796804\n",
      "359 0.0001705301838228479\n",
      "360 0.0001650621125008911\n",
      "361 0.00015977704606484622\n",
      "362 0.0001546599087305367\n",
      "363 0.00014970764459576458\n",
      "364 0.00014491476758848876\n",
      "365 0.0001402839698130265\n",
      "366 0.00013578923244494945\n",
      "367 0.00013144381227903068\n",
      "368 0.000127244638861157\n",
      "369 0.00012317407527007163\n",
      "370 0.00011924230784643441\n",
      "371 0.00011543121945578605\n",
      "372 0.00011174222163390368\n",
      "373 0.00010817791917361319\n",
      "374 0.00010474180453456938\n",
      "375 0.00010141210805159062\n",
      "376 9.817598038353026e-05\n",
      "377 9.504908666713163e-05\n",
      "378 9.202300861943513e-05\n",
      "379 8.909208554541692e-05\n",
      "380 8.625086775282398e-05\n",
      "381 8.350390271516517e-05\n",
      "382 8.08487311587669e-05\n",
      "383 7.827617082512006e-05\n",
      "384 7.57861853344366e-05\n",
      "385 7.337470742641017e-05\n",
      "386 7.104614633135498e-05\n",
      "387 6.878379645058885e-05\n",
      "388 6.660105282207951e-05\n",
      "389 6.448532803915441e-05\n",
      "390 6.243873212952167e-05\n",
      "391 6.045569170964882e-05\n",
      "392 5.853869515703991e-05\n",
      "393 5.668688754667528e-05\n",
      "394 5.489224713528529e-05\n",
      "395 5.3149833547649905e-05\n",
      "396 5.146827970747836e-05\n",
      "397 4.983628969057463e-05\n",
      "398 4.826188524020836e-05\n",
      "399 4.673657531384379e-05\n",
      "400 4.5258762838784605e-05\n",
      "401 4.382489714771509e-05\n",
      "402 4.2439281969564036e-05\n",
      "403 4.109870860702358e-05\n",
      "404 3.979693792643957e-05\n",
      "405 3.854016540572047e-05\n",
      "406 3.732333789230324e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407 3.614353045122698e-05\n",
      "408 3.500455568428151e-05\n",
      "409 3.389922494534403e-05\n",
      "410 3.28277783410158e-05\n",
      "411 3.179113264195621e-05\n",
      "412 3.0790517485002056e-05\n",
      "413 2.981757825182285e-05\n",
      "414 2.888053859351203e-05\n",
      "415 2.7969455913989805e-05\n",
      "416 2.7089503419119865e-05\n",
      "417 2.6234780307277106e-05\n",
      "418 2.540991954447236e-05\n",
      "419 2.4606855731690302e-05\n",
      "420 2.3833299565012567e-05\n",
      "421 2.308688090124633e-05\n",
      "422 2.2362448362400755e-05\n",
      "423 2.1663810912286863e-05\n",
      "424 2.0984669390600175e-05\n",
      "425 2.032906013482716e-05\n",
      "426 1.969210643437691e-05\n",
      "427 1.907606201712042e-05\n",
      "428 1.847874591476284e-05\n",
      "429 1.790308851923328e-05\n",
      "430 1.7343125364277512e-05\n",
      "431 1.6800071534817107e-05\n",
      "432 1.627391429792624e-05\n",
      "433 1.5766519936732948e-05\n",
      "434 1.5274372344720177e-05\n",
      "435 1.479650291003054e-05\n",
      "436 1.4333638318930753e-05\n",
      "437 1.3886739907320589e-05\n",
      "438 1.3454100553644821e-05\n",
      "439 1.3034778930887114e-05\n",
      "440 1.2626720490516163e-05\n",
      "441 1.2233856068633031e-05\n",
      "442 1.1853419891849626e-05\n",
      "443 1.148283263319172e-05\n",
      "444 1.1124589036626276e-05\n",
      "445 1.077733759302646e-05\n",
      "446 1.0442988241265994e-05\n",
      "447 1.0116954399563838e-05\n",
      "448 9.803003194974735e-06\n",
      "449 9.49810510064708e-06\n",
      "450 9.203044101013802e-06\n",
      "451 8.91693798621418e-06\n",
      "452 8.639784027764108e-06\n",
      "453 8.371422154596075e-06\n",
      "454 8.1102198237204e-06\n",
      "455 7.85896281740861e-06\n",
      "456 7.61437422625022e-06\n",
      "457 7.378764166787732e-06\n",
      "458 7.149012162699364e-06\n",
      "459 6.92728553985944e-06\n",
      "460 6.711655714752851e-06\n",
      "461 6.504651992145227e-06\n",
      "462 6.3015572777658235e-06\n",
      "463 6.1074479162925854e-06\n",
      "464 5.916801001148997e-06\n",
      "465 5.73490660826792e-06\n",
      "466 5.556798441830324e-06\n",
      "467 5.384999440138927e-06\n",
      "468 5.2189479902153835e-06\n",
      "469 5.056923782831291e-06\n",
      "470 4.9009095164365135e-06\n",
      "471 4.747997536469484e-06\n",
      "472 4.602021817845525e-06\n",
      "473 4.459213414520491e-06\n",
      "474 4.321525011619087e-06\n",
      "475 4.187621925666463e-06\n",
      "476 4.05859827878885e-06\n",
      "477 3.93328537029447e-06\n",
      "478 3.812244585787994e-06\n",
      "479 3.6951446418242995e-06\n",
      "480 3.5801147078018403e-06\n",
      "481 3.46910610460327e-06\n",
      "482 3.3625117339397548e-06\n",
      "483 3.258423475926975e-06\n",
      "484 3.158314029860776e-06\n",
      "485 3.060595645365538e-06\n",
      "486 2.9669859031855594e-06\n",
      "487 2.8758183816535166e-06\n",
      "488 2.7862333809025586e-06\n",
      "489 2.7003204650100088e-06\n",
      "490 2.6169877855863888e-06\n",
      "491 2.536457259338931e-06\n",
      "492 2.458545850458904e-06\n",
      "493 2.3830093596188817e-06\n",
      "494 2.3092156880011316e-06\n",
      "495 2.2381088911060942e-06\n",
      "496 2.1693858798244037e-06\n",
      "497 2.1029284198448295e-06\n",
      "498 2.0377806322358083e-06\n",
      "499 1.975223995032138e-06\n"
     ]
    }
   ],
   "source": [
    "#nn 패키지의 활용(ppt 16p)\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의합니다.\n",
    "# nn.Sequential은 다른 Module들을 포함하는 Module로, 그 Module들을 순차적으로\n",
    "# 적용하여 출력을 생성합니다. 각각의 Linear Module은 선형 함수를 사용하여\n",
    "# 입력으로부터 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# 또한 nn 패키지에는 널리 사용하는 손실 함수들에 대한 정의도 포함하고 있습니다;\n",
    "# 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다. Module 객체는\n",
    "    # __call__ 연산자를 덮어써(override) 함수처럼 호출할 수 있게 합니다.\n",
    "    # 이렇게 함으로써 입력 데이터의 Tensor를 Module에 전달하여 출력 데이터의\n",
    "    # Tensor를 생성합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다. 예측한 y와 정답인 y를 갖는 Tensor들을 전달하고,\n",
    "    # 손실 함수는 손실 값을 갖는 Tensor를 반환합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계를 실행하기 전에 변화도를 0으로 만듭니다.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를\n",
    "    # 계산합니다. 내부적으로 각 Module의 매개변수는 requires_grad=True 일 때\n",
    "    # Tensor 내에 저장되므로, 이 호출은 모든 모델의 모든 학습 가능한 매개변수의\n",
    "    # 변화도를 계산하게 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다. 각 매개변수는\n",
    "    # Tensor이므로 이전에 했던 것과 같이 변화도에 접근할 수 있습니다.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 718.328857421875\n",
      "1 700.1650390625\n",
      "2 682.463623046875\n",
      "3 665.3062744140625\n",
      "4 648.6083984375\n",
      "5 632.4580688476562\n",
      "6 616.7861328125\n",
      "7 601.5977783203125\n",
      "8 586.843017578125\n",
      "9 572.5950927734375\n",
      "10 558.820068359375\n",
      "11 545.4866943359375\n",
      "12 532.505615234375\n",
      "13 519.8849487304688\n",
      "14 507.5215148925781\n",
      "15 495.45001220703125\n",
      "16 483.7677001953125\n",
      "17 472.4457092285156\n",
      "18 461.4499816894531\n",
      "19 450.74163818359375\n",
      "20 440.27130126953125\n",
      "21 430.1374816894531\n",
      "22 420.28070068359375\n",
      "23 410.6038818359375\n",
      "24 401.1672058105469\n",
      "25 391.9395751953125\n",
      "26 382.9339599609375\n",
      "27 374.1667785644531\n",
      "28 365.5720520019531\n",
      "29 357.1414794921875\n",
      "30 348.8700866699219\n",
      "31 340.7496337890625\n",
      "32 332.83758544921875\n",
      "33 325.09619140625\n",
      "34 317.49554443359375\n",
      "35 310.0271911621094\n",
      "36 302.7166442871094\n",
      "37 295.5619812011719\n",
      "38 288.54620361328125\n",
      "39 281.7073974609375\n",
      "40 275.0262756347656\n",
      "41 268.47515869140625\n",
      "42 262.04205322265625\n",
      "43 255.73312377929688\n",
      "44 249.59234619140625\n",
      "45 243.5795135498047\n",
      "46 237.67288208007812\n",
      "47 231.86883544921875\n",
      "48 226.1829071044922\n",
      "49 220.60589599609375\n",
      "50 215.1186981201172\n",
      "51 209.7523956298828\n",
      "52 204.51419067382812\n",
      "53 199.3831024169922\n",
      "54 194.3645782470703\n",
      "55 189.44215393066406\n",
      "56 184.61672973632812\n",
      "57 179.8896942138672\n",
      "58 175.24807739257812\n",
      "59 170.70596313476562\n",
      "60 166.25845336914062\n",
      "61 161.91810607910156\n",
      "62 157.6705322265625\n",
      "63 153.52642822265625\n",
      "64 149.46817016601562\n",
      "65 145.50193786621094\n",
      "66 141.6248779296875\n",
      "67 137.83087158203125\n",
      "68 134.1111602783203\n",
      "69 130.4658660888672\n",
      "70 126.89335632324219\n",
      "71 123.40074157714844\n",
      "72 119.98733520507812\n",
      "73 116.64942169189453\n",
      "74 113.38919067382812\n",
      "75 110.19708251953125\n",
      "76 107.07415771484375\n",
      "77 104.02015686035156\n",
      "78 101.0328369140625\n",
      "79 98.11149597167969\n",
      "80 95.25605773925781\n",
      "81 92.46756744384766\n",
      "82 89.74101257324219\n",
      "83 87.08324432373047\n",
      "84 84.48823547363281\n",
      "85 81.95252227783203\n",
      "86 79.47736358642578\n",
      "87 77.06163787841797\n",
      "88 74.70123291015625\n",
      "89 72.39996337890625\n",
      "90 70.15169525146484\n",
      "91 67.95979309082031\n",
      "92 65.82310485839844\n",
      "93 63.73823928833008\n",
      "94 61.706939697265625\n",
      "95 59.7279052734375\n",
      "96 57.8017578125\n",
      "97 55.92473220825195\n",
      "98 54.09613037109375\n",
      "99 52.31711196899414\n",
      "100 50.5828742980957\n",
      "101 48.897335052490234\n",
      "102 47.25618362426758\n",
      "103 45.65975570678711\n",
      "104 44.10664749145508\n",
      "105 42.599117279052734\n",
      "106 41.13351058959961\n",
      "107 39.709163665771484\n",
      "108 38.32335662841797\n",
      "109 36.978912353515625\n",
      "110 35.67531204223633\n",
      "111 34.406005859375\n",
      "112 33.17247009277344\n",
      "113 31.97442626953125\n",
      "114 30.812366485595703\n",
      "115 29.68372344970703\n",
      "116 28.590360641479492\n",
      "117 27.53290557861328\n",
      "118 26.506561279296875\n",
      "119 25.515535354614258\n",
      "120 24.557165145874023\n",
      "121 23.627140045166016\n",
      "122 22.72858428955078\n",
      "123 21.85984992980957\n",
      "124 21.020158767700195\n",
      "125 20.20700454711914\n",
      "126 19.421112060546875\n",
      "127 18.660858154296875\n",
      "128 17.92668914794922\n",
      "129 17.219205856323242\n",
      "130 16.535499572753906\n",
      "131 15.876921653747559\n",
      "132 15.240221977233887\n",
      "133 14.626574516296387\n",
      "134 14.03626537322998\n",
      "135 13.467286109924316\n",
      "136 12.919278144836426\n",
      "137 12.390177726745605\n",
      "138 11.880936622619629\n",
      "139 11.390270233154297\n",
      "140 10.918269157409668\n",
      "141 10.464252471923828\n",
      "142 10.027047157287598\n",
      "143 9.6063232421875\n",
      "144 9.202018737792969\n",
      "145 8.812640190124512\n",
      "146 8.438488006591797\n",
      "147 8.078123092651367\n",
      "148 7.732090950012207\n",
      "149 7.399306774139404\n",
      "150 7.079187393188477\n",
      "151 6.771539688110352\n",
      "152 6.476442813873291\n",
      "153 6.193566799163818\n",
      "154 5.921407699584961\n",
      "155 5.660905361175537\n",
      "156 5.411308765411377\n",
      "157 5.170871257781982\n",
      "158 4.940451145172119\n",
      "159 4.719605922698975\n",
      "160 4.508239269256592\n",
      "161 4.305583953857422\n",
      "162 4.111587047576904\n",
      "163 3.92543888092041\n",
      "164 3.747467041015625\n",
      "165 3.576860189437866\n",
      "166 3.413536548614502\n",
      "167 3.2573015689849854\n",
      "168 3.107619285583496\n",
      "169 2.964524507522583\n",
      "170 2.8276257514953613\n",
      "171 2.696849822998047\n",
      "172 2.571617603302002\n",
      "173 2.4519054889678955\n",
      "174 2.337585210800171\n",
      "175 2.2282602787017822\n",
      "176 2.1237215995788574\n",
      "177 2.0237956047058105\n",
      "178 1.928359866142273\n",
      "179 1.8372066020965576\n",
      "180 1.750117540359497\n",
      "181 1.6668856143951416\n",
      "182 1.5875122547149658\n",
      "183 1.5115221738815308\n",
      "184 1.4389058351516724\n",
      "185 1.3694875240325928\n",
      "186 1.3033159971237183\n",
      "187 1.240057349205017\n",
      "188 1.179758071899414\n",
      "189 1.122137427330017\n",
      "190 1.0671746730804443\n",
      "191 1.0147043466567993\n",
      "192 0.9646410346031189\n",
      "193 0.9169491529464722\n",
      "194 0.871467113494873\n",
      "195 0.8282204270362854\n",
      "196 0.7869551181793213\n",
      "197 0.7476767897605896\n",
      "198 0.7103239893913269\n",
      "199 0.6747162342071533\n",
      "200 0.6408181190490723\n",
      "201 0.6085084080696106\n",
      "202 0.5778141617774963\n",
      "203 0.5485906004905701\n",
      "204 0.5208103656768799\n",
      "205 0.4943690598011017\n",
      "206 0.46922892332077026\n",
      "207 0.44532716274261475\n",
      "208 0.42259857058525085\n",
      "209 0.4010070860385895\n",
      "210 0.3804570436477661\n",
      "211 0.360953152179718\n",
      "212 0.34240400791168213\n",
      "213 0.32477185130119324\n",
      "214 0.3080204129219055\n",
      "215 0.29209306836128235\n",
      "216 0.2769761085510254\n",
      "217 0.2626159191131592\n",
      "218 0.2490220069885254\n",
      "219 0.23610419034957886\n",
      "220 0.22385115921497345\n",
      "221 0.21221795678138733\n",
      "222 0.20119281113147736\n",
      "223 0.1906992495059967\n",
      "224 0.18075339496135712\n",
      "225 0.17136429250240326\n",
      "226 0.16245171427726746\n",
      "227 0.15399599075317383\n",
      "228 0.14598515629768372\n",
      "229 0.13837489485740662\n",
      "230 0.1311665028333664\n",
      "231 0.12432616949081421\n",
      "232 0.11783838272094727\n",
      "233 0.11169368773698807\n",
      "234 0.10586263984441757\n",
      "235 0.1003340631723404\n",
      "236 0.09509037435054779\n",
      "237 0.0901237204670906\n",
      "238 0.08541012555360794\n",
      "239 0.08094290643930435\n",
      "240 0.0767107680439949\n",
      "241 0.07269678264856339\n",
      "242 0.06889259815216064\n",
      "243 0.06528359651565552\n",
      "244 0.06186240538954735\n",
      "245 0.05862177163362503\n",
      "246 0.05554857850074768\n",
      "247 0.05263597518205643\n",
      "248 0.04987429827451706\n",
      "249 0.04725702852010727\n",
      "250 0.04477676376700401\n",
      "251 0.042425718158483505\n",
      "252 0.04019661247730255\n",
      "253 0.03808511421084404\n",
      "254 0.03608233854174614\n",
      "255 0.03418464586138725\n",
      "256 0.03238585963845253\n",
      "257 0.030681224539875984\n",
      "258 0.029067013412714005\n",
      "259 0.027534954249858856\n",
      "260 0.026082217693328857\n",
      "261 0.024707097560167313\n",
      "262 0.02340388484299183\n",
      "263 0.02216983400285244\n",
      "264 0.020999891683459282\n",
      "265 0.01989089325070381\n",
      "266 0.018839731812477112\n",
      "267 0.017843885347247124\n",
      "268 0.01689998060464859\n",
      "269 0.01600579358637333\n",
      "270 0.01515831146389246\n",
      "271 0.014355376362800598\n",
      "272 0.01359472330659628\n",
      "273 0.01287414412945509\n",
      "274 0.012191168032586575\n",
      "275 0.011544250883162022\n",
      "276 0.010930906049907207\n",
      "277 0.010350102558732033\n",
      "278 0.009800084866583347\n",
      "279 0.009278904646635056\n",
      "280 0.008785014040768147\n",
      "281 0.008317273110151291\n",
      "282 0.007874155417084694\n",
      "283 0.007454437669366598\n",
      "284 0.007056828588247299\n",
      "285 0.0066802590154111385\n",
      "286 0.0063234297558665276\n",
      "287 0.005985450465232134\n",
      "288 0.0056654904037714005\n",
      "289 0.005362443160265684\n",
      "290 0.005075337365269661\n",
      "291 0.0048033734783530235\n",
      "292 0.004545945208519697\n",
      "293 0.004302063025534153\n",
      "294 0.004071142990142107\n",
      "295 0.0038524840492755175\n",
      "296 0.003645810764282942\n",
      "297 0.0034495166037231684\n",
      "298 0.0032640122808516026\n",
      "299 0.0030883378349244595\n",
      "300 0.0029222548473626375\n",
      "301 0.002764607546851039\n",
      "302 0.002615493256598711\n",
      "303 0.0024743997491896152\n",
      "304 0.0023407775443047285\n",
      "305 0.002214303007349372\n",
      "306 0.0020945605356246233\n",
      "307 0.0019812514074146748\n",
      "308 0.0018739866791293025\n",
      "309 0.0017724272329360247\n",
      "310 0.0016762481536716223\n",
      "311 0.0015852517681196332\n",
      "312 0.0014991420321166515\n",
      "313 0.001417640596628189\n",
      "314 0.0013405424542725086\n",
      "315 0.0012674847384914756\n",
      "316 0.0011984369484707713\n",
      "317 0.001133095589466393\n",
      "318 0.0010712279472500086\n",
      "319 0.0010127249406650662\n",
      "320 0.0009573677671141922\n",
      "321 0.0009050141088664532\n",
      "322 0.0008554443484172225\n",
      "323 0.0008085628505796194\n",
      "324 0.0007642473792657256\n",
      "325 0.0007223165011964738\n",
      "326 0.0006826262106187642\n",
      "327 0.0006451279041357338\n",
      "328 0.0006096501601859927\n",
      "329 0.0005760862259194255\n",
      "330 0.000544356822501868\n",
      "331 0.0005143514717929065\n",
      "332 0.00048597503337077796\n",
      "333 0.000459144328488037\n",
      "334 0.0004337866557762027\n",
      "335 0.00040979916229844093\n",
      "336 0.00038712017703801394\n",
      "337 0.0003656878834590316\n",
      "338 0.0003454268735367805\n",
      "339 0.0003262718964833766\n",
      "340 0.00030817047809250653\n",
      "341 0.00029106010333634913\n",
      "342 0.00027488861815072596\n",
      "343 0.0002595996775198728\n",
      "344 0.0002451675245538354\n",
      "345 0.00023150972265284508\n",
      "346 0.000218617933569476\n",
      "347 0.00020642649906221777\n",
      "348 0.0001949112192960456\n",
      "349 0.00018403440481051803\n",
      "350 0.0001737552956910804\n",
      "351 0.00016403518384322524\n",
      "352 0.00015486583288293332\n",
      "353 0.00014620297588407993\n",
      "354 0.0001380120520479977\n",
      "355 0.00013028080866206437\n",
      "356 0.00012297290959395468\n",
      "357 0.0001160805913968943\n",
      "358 0.00010956612095469609\n",
      "359 0.00010341156303184107\n",
      "360 9.759829117683694e-05\n",
      "361 9.211141150444746e-05\n",
      "362 8.693089330336079e-05\n",
      "363 8.204361802199855e-05\n",
      "364 7.742070738459006e-05\n",
      "365 7.306205225177109e-05\n",
      "366 6.894266698509455e-05\n",
      "367 6.505859346361831e-05\n",
      "368 6.138471508165821e-05\n",
      "369 5.7919642131309956e-05\n",
      "370 5.4651638492941856e-05\n",
      "371 5.156587576493621e-05\n",
      "372 4.865435403189622e-05\n",
      "373 4.590030948747881e-05\n",
      "374 4.330677620600909e-05\n",
      "375 4.0854316466720775e-05\n",
      "376 3.854435635730624e-05\n",
      "377 3.636226392700337e-05\n",
      "378 3.430199285503477e-05\n",
      "379 3.2359337637899444e-05\n",
      "380 3.052626198041253e-05\n",
      "381 2.879559360735584e-05\n",
      "382 2.716189919738099e-05\n",
      "383 2.5624276531743817e-05\n",
      "384 2.416746792732738e-05\n",
      "385 2.2798973077442497e-05\n",
      "386 2.1502897652680986e-05\n",
      "387 2.0283161575207487e-05\n",
      "388 1.9133227397105657e-05\n",
      "389 1.8044200260192156e-05\n",
      "390 1.702177178231068e-05\n",
      "391 1.6053776562330313e-05\n",
      "392 1.5140808500291314e-05\n",
      "393 1.4280548384704161e-05\n",
      "394 1.3468007637129631e-05\n",
      "395 1.270277425646782e-05\n",
      "396 1.1980995623162016e-05\n",
      "397 1.1300011465209536e-05\n",
      "398 1.0656866834324319e-05\n",
      "399 1.005131343845278e-05\n",
      "400 9.479563232162036e-06\n",
      "401 8.939973668020684e-06\n",
      "402 8.433067705482244e-06\n",
      "403 7.951422048790846e-06\n",
      "404 7.499907042074483e-06\n",
      "405 7.073366759868804e-06\n",
      "406 6.6716565925162286e-06\n",
      "407 6.291890258580679e-06\n",
      "408 5.933809916314203e-06\n",
      "409 5.596768005489139e-06\n",
      "410 5.278357548377244e-06\n",
      "411 4.977195658284472e-06\n",
      "412 4.694043582276208e-06\n",
      "413 4.4265957512834575e-06\n",
      "414 4.1754265112103894e-06\n",
      "415 3.9374581319862045e-06\n",
      "416 3.7136903756618267e-06\n",
      "417 3.5032278447033605e-06\n",
      "418 3.3036653803719673e-06\n",
      "419 3.1156587283476256e-06\n",
      "420 2.9387904305622214e-06\n",
      "421 2.7714945645129774e-06\n",
      "422 2.6137938675674377e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423 2.4657263111294014e-06\n",
      "424 2.3256231997947907e-06\n",
      "425 2.1932492018095218e-06\n",
      "426 2.0686957213911228e-06\n",
      "427 1.951805870703538e-06\n",
      "428 1.8403391095489496e-06\n",
      "429 1.7358910326947807e-06\n",
      "430 1.6369913282687776e-06\n",
      "431 1.5436511375810369e-06\n",
      "432 1.4564304819941754e-06\n",
      "433 1.3735216271015815e-06\n",
      "434 1.2953736359122558e-06\n",
      "435 1.2220525604789145e-06\n",
      "436 1.152091726908111e-06\n",
      "437 1.0871854101424105e-06\n",
      "438 1.0254560720568406e-06\n",
      "439 9.672974101704312e-07\n",
      "440 9.123135669142357e-07\n",
      "441 8.603460628364701e-07\n",
      "442 8.115668492791883e-07\n",
      "443 7.656529987798422e-07\n",
      "444 7.217982442853099e-07\n",
      "445 6.810350328123604e-07\n",
      "446 6.424589287234994e-07\n",
      "447 6.059262318558467e-07\n",
      "448 5.715043016607524e-07\n",
      "449 5.388465069700032e-07\n",
      "450 5.085389602754731e-07\n",
      "451 4.798245072379359e-07\n",
      "452 4.5222267885947076e-07\n",
      "453 4.2666147237468977e-07\n",
      "454 4.025187649858708e-07\n",
      "455 3.7965517662996717e-07\n",
      "456 3.5790000652013987e-07\n",
      "457 3.3777615726648946e-07\n",
      "458 3.184989907367708e-07\n",
      "459 3.0043645438126987e-07\n",
      "460 2.8337032631498005e-07\n",
      "461 2.6733101776699186e-07\n",
      "462 2.521836393043486e-07\n",
      "463 2.3795676895588258e-07\n",
      "464 2.2432797663896054e-07\n",
      "465 2.1165621433283377e-07\n",
      "466 1.997249938767709e-07\n",
      "467 1.883753952824918e-07\n",
      "468 1.7772357807643857e-07\n",
      "469 1.6760878907007282e-07\n",
      "470 1.581859834232091e-07\n",
      "471 1.4921410240731348e-07\n",
      "472 1.4074852572321106e-07\n",
      "473 1.3273736954033666e-07\n",
      "474 1.2524296266747115e-07\n",
      "475 1.181268842742611e-07\n",
      "476 1.1139779587665544e-07\n",
      "477 1.0505112157943586e-07\n",
      "478 9.915956411532534e-08\n",
      "479 9.34394321916443e-08\n",
      "480 8.825451658367456e-08\n",
      "481 8.336107981676832e-08\n",
      "482 7.855403794110316e-08\n",
      "483 7.413320446403304e-08\n",
      "484 6.987900036392602e-08\n",
      "485 6.59856738138842e-08\n",
      "486 6.218775183697289e-08\n",
      "487 5.876060527043592e-08\n",
      "488 5.537608060990351e-08\n",
      "489 5.21499750050225e-08\n",
      "490 4.935880681955496e-08\n",
      "491 4.6502151462846086e-08\n",
      "492 4.386883034612765e-08\n",
      "493 4.134327369342827e-08\n",
      "494 3.897126532592665e-08\n",
      "495 3.676155913012735e-08\n",
      "496 3.474538345926703e-08\n",
      "497 3.2714151387835955e-08\n",
      "498 3.084382882434511e-08\n",
      "499 2.9079808783194494e-08\n"
     ]
    }
   ],
   "source": [
    "#optim 패키지가 제공하는 adam 옵티마이저 활용(ppt 17p)\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델과 손실 함수를 정의합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# optim 패키지를 사용하여 모델의 가중치를 갱신할 Optimizer를 정의합니다.\n",
    "# 여기서는 Adam을 사용하겠습니다; optim 패키지는 다른 다양한 최적화 알고리즘을\n",
    "# 포함하고 있습니다. Adam 생성자의 첫번째 인자는 어떤 Tensor가 갱신되어야 하는지\n",
    "# 알려줍니다.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계 전에, Optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인)\n",
    "    # 갱신할 변수들에 대한 모든 변화도를 0으로 만듭니다. 이렇게 하는 이유는\n",
    "    # 기본적으로 .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고)\n",
    "    # 누적되기 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를\n",
    "    # 참조하세요.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 매개변수에 대한 손실의 변화도를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 691.5057373046875\n",
      "1 643.7177124023438\n",
      "2 601.8855590820312\n",
      "3 565.20068359375\n",
      "4 532.5328979492188\n",
      "5 502.78326416015625\n",
      "6 475.60821533203125\n",
      "7 450.4493103027344\n",
      "8 426.9801330566406\n",
      "9 404.99755859375\n",
      "10 384.3141174316406\n",
      "11 364.8726806640625\n",
      "12 346.44598388671875\n",
      "13 328.7873840332031\n",
      "14 311.935791015625\n",
      "15 295.75994873046875\n",
      "16 280.4377136230469\n",
      "17 265.8594970703125\n",
      "18 251.931396484375\n",
      "19 238.6143341064453\n",
      "20 225.88795471191406\n",
      "21 213.7564239501953\n",
      "22 202.14761352539062\n",
      "23 191.027099609375\n",
      "24 180.39337158203125\n",
      "25 170.24819946289062\n",
      "26 160.61932373046875\n",
      "27 151.45521545410156\n",
      "28 142.72596740722656\n",
      "29 134.4398956298828\n",
      "30 126.56427001953125\n",
      "31 119.0953369140625\n",
      "32 112.03524780273438\n",
      "33 105.36739349365234\n",
      "34 99.06084442138672\n",
      "35 93.10750579833984\n",
      "36 87.48711395263672\n",
      "37 82.18421173095703\n",
      "38 77.20060729980469\n",
      "39 72.52629852294922\n",
      "40 68.12267303466797\n",
      "41 63.976722717285156\n",
      "42 60.08894729614258\n",
      "43 56.43720626831055\n",
      "44 53.01973342895508\n",
      "45 49.8038215637207\n",
      "46 46.79058074951172\n",
      "47 43.96930694580078\n",
      "48 41.32548141479492\n",
      "49 38.84697723388672\n",
      "50 36.529876708984375\n",
      "51 34.362510681152344\n",
      "52 32.33311080932617\n",
      "53 30.432804107666016\n",
      "54 28.655668258666992\n",
      "55 26.989322662353516\n",
      "56 25.428850173950195\n",
      "57 23.968034744262695\n",
      "58 22.600536346435547\n",
      "59 21.317638397216797\n",
      "60 20.115264892578125\n",
      "61 18.988109588623047\n",
      "62 17.930461883544922\n",
      "63 16.936405181884766\n",
      "64 16.003541946411133\n",
      "65 15.127236366271973\n",
      "66 14.304387092590332\n",
      "67 13.532564163208008\n",
      "68 12.806843757629395\n",
      "69 12.124455451965332\n",
      "70 11.483420372009277\n",
      "71 10.879390716552734\n",
      "72 10.312321662902832\n",
      "73 9.778196334838867\n",
      "74 9.274765014648438\n",
      "75 8.799989700317383\n",
      "76 8.351922035217285\n",
      "77 7.929549694061279\n",
      "78 7.530463695526123\n",
      "79 7.153123378753662\n",
      "80 6.797365665435791\n",
      "81 6.4609808921813965\n",
      "82 6.142971515655518\n",
      "83 5.8420186042785645\n",
      "84 5.558032035827637\n",
      "85 5.28947639465332\n",
      "86 5.034949779510498\n",
      "87 4.795112133026123\n",
      "88 4.567826271057129\n",
      "89 4.3526434898376465\n",
      "90 4.148962020874023\n",
      "91 3.9557411670684814\n",
      "92 3.772270441055298\n",
      "93 3.59800386428833\n",
      "94 3.432347536087036\n",
      "95 3.274965763092041\n",
      "96 3.125572443008423\n",
      "97 2.9835100173950195\n",
      "98 2.8485422134399414\n",
      "99 2.720165729522705\n",
      "100 2.5980401039123535\n",
      "101 2.482041597366333\n",
      "102 2.3717823028564453\n",
      "103 2.266958713531494\n",
      "104 2.167213201522827\n",
      "105 2.0722293853759766\n",
      "106 1.9817978143692017\n",
      "107 1.895606279373169\n",
      "108 1.8134398460388184\n",
      "109 1.735190510749817\n",
      "110 1.6605064868927002\n",
      "111 1.5893077850341797\n",
      "112 1.5214078426361084\n",
      "113 1.4566196203231812\n",
      "114 1.394788384437561\n",
      "115 1.3357919454574585\n",
      "116 1.2794225215911865\n",
      "117 1.2255741357803345\n",
      "118 1.1741745471954346\n",
      "119 1.125144124031067\n",
      "120 1.0783613920211792\n",
      "121 1.0337855815887451\n",
      "122 0.9910880923271179\n",
      "123 0.9503114223480225\n",
      "124 0.9114592671394348\n",
      "125 0.8743053078651428\n",
      "126 0.8387854695320129\n",
      "127 0.8047999739646912\n",
      "128 0.7722916007041931\n",
      "129 0.7412217259407043\n",
      "130 0.7114116549491882\n",
      "131 0.6828883290290833\n",
      "132 0.655593752861023\n",
      "133 0.6294714212417603\n",
      "134 0.6044550538063049\n",
      "135 0.5805032253265381\n",
      "136 0.5575676560401917\n",
      "137 0.5355971455574036\n",
      "138 0.5145800709724426\n",
      "139 0.494396448135376\n",
      "140 0.4750465452671051\n",
      "141 0.45648497343063354\n",
      "142 0.43870809674263\n",
      "143 0.421658456325531\n",
      "144 0.4052969515323639\n",
      "145 0.3896014094352722\n",
      "146 0.37455224990844727\n",
      "147 0.36011892557144165\n",
      "148 0.34627264738082886\n",
      "149 0.33299198746681213\n",
      "150 0.3202272355556488\n",
      "151 0.3079824447631836\n",
      "152 0.29625165462493896\n",
      "153 0.2849791347980499\n",
      "154 0.27415621280670166\n",
      "155 0.26377904415130615\n",
      "156 0.25380343198776245\n",
      "157 0.24421463906764984\n",
      "158 0.23500189185142517\n",
      "159 0.22615118324756622\n",
      "160 0.21765682101249695\n",
      "161 0.20949023962020874\n",
      "162 0.20163904130458832\n",
      "163 0.19410236179828644\n",
      "164 0.18685650825500488\n",
      "165 0.1798928678035736\n",
      "166 0.17320303618907928\n",
      "167 0.16676850616931915\n",
      "168 0.16058601438999176\n",
      "169 0.15463858842849731\n",
      "170 0.1489175707101822\n",
      "171 0.14342303574085236\n",
      "172 0.13814227283000946\n",
      "173 0.13306353986263275\n",
      "174 0.1281730979681015\n",
      "175 0.12347231060266495\n",
      "176 0.11895499378442764\n",
      "177 0.11461092531681061\n",
      "178 0.11042767763137817\n",
      "179 0.10640369355678558\n",
      "180 0.10253947228193283\n",
      "181 0.09882398694753647\n",
      "182 0.09524009376764297\n",
      "183 0.09179454296827316\n",
      "184 0.08847539871931076\n",
      "185 0.08528100699186325\n",
      "186 0.08220555633306503\n",
      "187 0.07924582809209824\n",
      "188 0.07639654725790024\n",
      "189 0.07365400344133377\n",
      "190 0.07101256400346756\n",
      "191 0.06846801191568375\n",
      "192 0.06601881235837936\n",
      "193 0.06366249918937683\n",
      "194 0.06139219179749489\n",
      "195 0.05920404940843582\n",
      "196 0.05709878355264664\n",
      "197 0.0550694577395916\n",
      "198 0.05311436951160431\n",
      "199 0.05123206973075867\n",
      "200 0.049419041723012924\n",
      "201 0.047672782093286514\n",
      "202 0.04598930850625038\n",
      "203 0.04436936974525452\n",
      "204 0.04280756413936615\n",
      "205 0.041301287710666656\n",
      "206 0.039849843829870224\n",
      "207 0.03845078870654106\n",
      "208 0.037105217576026917\n",
      "209 0.03580743819475174\n",
      "210 0.03455565124750137\n",
      "211 0.03334889933466911\n",
      "212 0.03218567371368408\n",
      "213 0.03106422908604145\n",
      "214 0.02998233400285244\n",
      "215 0.028940118849277496\n",
      "216 0.02793576382100582\n",
      "217 0.026966068893671036\n",
      "218 0.02603192813694477\n",
      "219 0.02513134852051735\n",
      "220 0.02426265925168991\n",
      "221 0.023424256592988968\n",
      "222 0.022615674883127213\n",
      "223 0.02183578535914421\n",
      "224 0.02108384110033512\n",
      "225 0.020358648151159286\n",
      "226 0.019659001380205154\n",
      "227 0.01898413896560669\n",
      "228 0.018333081156015396\n",
      "229 0.01770550198853016\n",
      "230 0.01709994301199913\n",
      "231 0.016515295952558517\n",
      "232 0.015951192006468773\n",
      "233 0.01540677435696125\n",
      "234 0.014882749877870083\n",
      "235 0.014376780018210411\n",
      "236 0.013887547887861729\n",
      "237 0.013415695168077946\n",
      "238 0.012960237450897694\n",
      "239 0.012520828284323215\n",
      "240 0.012096746824681759\n",
      "241 0.011687164194881916\n",
      "242 0.011291979812085629\n",
      "243 0.010910733602941036\n",
      "244 0.010542557574808598\n",
      "245 0.010187049396336079\n",
      "246 0.009844168089330196\n",
      "247 0.009512994438409805\n",
      "248 0.009193096309900284\n",
      "249 0.008884293958544731\n",
      "250 0.008586153388023376\n",
      "251 0.008298173546791077\n",
      "252 0.008020102977752686\n",
      "253 0.007751501630991697\n",
      "254 0.007492386270314455\n",
      "255 0.007242025341838598\n",
      "256 0.007000327575951815\n",
      "257 0.006766894832253456\n",
      "258 0.006541439797729254\n",
      "259 0.006323766428977251\n",
      "260 0.006113974843174219\n",
      "261 0.005910774227231741\n",
      "262 0.005714554339647293\n",
      "263 0.005524929612874985\n",
      "264 0.0053418721072375774\n",
      "265 0.00516492547467351\n",
      "266 0.004994064569473267\n",
      "267 0.00482898810878396\n",
      "268 0.00466933473944664\n",
      "269 0.004515236709266901\n",
      "270 0.004366394132375717\n",
      "271 0.004222442861646414\n",
      "272 0.004083402454853058\n",
      "273 0.003948999103158712\n",
      "274 0.0038193874061107635\n",
      "275 0.0036939021665602922\n",
      "276 0.0035726698115468025\n",
      "277 0.0034554656594991684\n",
      "278 0.0033422093838453293\n",
      "279 0.0032328309025615454\n",
      "280 0.0031270214822143316\n",
      "281 0.003024887992069125\n",
      "282 0.002926065819337964\n",
      "283 0.0028305649757385254\n",
      "284 0.002738445531576872\n",
      "285 0.0026492734905332327\n",
      "286 0.0025629736483097076\n",
      "287 0.002479619113728404\n",
      "288 0.0023990143090486526\n",
      "289 0.002321084728464484\n",
      "290 0.0022457214072346687\n",
      "291 0.002172868000343442\n",
      "292 0.0021024225279688835\n",
      "293 0.002034317934885621\n",
      "294 0.0019685039296746254\n",
      "295 0.0019048601388931274\n",
      "296 0.0018433132208883762\n",
      "297 0.0017838604981079698\n",
      "298 0.0017262604087591171\n",
      "299 0.0016705647576600313\n",
      "300 0.0016167620196938515\n",
      "301 0.0015647014370188117\n",
      "302 0.001514358795247972\n",
      "303 0.001465663081035018\n",
      "304 0.0014185928739607334\n",
      "305 0.0013730470091104507\n",
      "306 0.001328975078649819\n",
      "307 0.0012863569427281618\n",
      "308 0.001245276303961873\n",
      "309 0.0012054236140102148\n",
      "310 0.0011668934021145105\n",
      "311 0.0011295884614810348\n",
      "312 0.0010935156606137753\n",
      "313 0.0010586136486381292\n",
      "314 0.001024839235469699\n",
      "315 0.0009921840392053127\n",
      "316 0.000960573845077306\n",
      "317 0.0009300144156441092\n",
      "318 0.000900417857337743\n",
      "319 0.0008718087337911129\n",
      "320 0.0008441010722890496\n",
      "321 0.0008173174574039876\n",
      "322 0.0007913833251222968\n",
      "323 0.0007662859279662371\n",
      "324 0.0007420023903250694\n",
      "325 0.0007185222348198295\n",
      "326 0.0006957862642593682\n",
      "327 0.000673806294798851\n",
      "328 0.0006525146891362965\n",
      "329 0.0006318933446891606\n",
      "330 0.0006119708414189517\n",
      "331 0.0005927016027271748\n",
      "332 0.000574024161323905\n",
      "333 0.000555948237888515\n",
      "334 0.0005384465912356973\n",
      "335 0.0005215209675952792\n",
      "336 0.0005051222397014499\n",
      "337 0.0004892618162557483\n",
      "338 0.0004738945863209665\n",
      "339 0.0004590211028698832\n",
      "340 0.0004446245147846639\n",
      "341 0.0004306861956138164\n",
      "342 0.00041720253648236394\n",
      "343 0.00040414684917777777\n",
      "344 0.0003915119159501046\n",
      "345 0.00037927215453237295\n",
      "346 0.0003674317558761686\n",
      "347 0.00035594901419244707\n",
      "348 0.00034484261414036155\n",
      "349 0.00033410100149922073\n",
      "350 0.00032368249958381057\n",
      "351 0.0003135953738819808\n",
      "352 0.00030383438570424914\n",
      "353 0.0002944065781775862\n",
      "354 0.0002852517063729465\n",
      "355 0.00027640178450383246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 0.00026781007181853056\n",
      "357 0.00025949289556592703\n",
      "358 0.0002514401567168534\n",
      "359 0.0002436433278489858\n",
      "360 0.000236092324485071\n",
      "361 0.0002287931856699288\n",
      "362 0.0002217062283307314\n",
      "363 0.00021484833268914372\n",
      "364 0.00020820424833800644\n",
      "365 0.00020177100668661296\n",
      "366 0.00019554125901777297\n",
      "367 0.0001894989109132439\n",
      "368 0.000183652009582147\n",
      "369 0.00017799093620851636\n",
      "370 0.00017250789096578956\n",
      "371 0.00016719575796741992\n",
      "372 0.0001620538969291374\n",
      "373 0.00015706273552495986\n",
      "374 0.00015224210801534355\n",
      "375 0.00014757057942915708\n",
      "376 0.00014303886564448476\n",
      "377 0.00013863926869817078\n",
      "378 0.00013439082249533385\n",
      "379 0.00013026931264903396\n",
      "380 0.00012627353135030717\n",
      "381 0.00012240666546858847\n",
      "382 0.00011865796113852412\n",
      "383 0.00011502570851007476\n",
      "384 0.00011151083162985742\n",
      "385 0.00010810570529429242\n",
      "386 0.00010479718184797093\n",
      "387 0.00010159837256651372\n",
      "388 9.849566413322464e-05\n",
      "389 9.548937669023871e-05\n",
      "390 9.258468344341964e-05\n",
      "391 8.976207755040377e-05\n",
      "392 8.702589548192918e-05\n",
      "393 8.438254008069634e-05\n",
      "394 8.181111479643732e-05\n",
      "395 7.932276639621705e-05\n",
      "396 7.691258360864595e-05\n",
      "397 7.457521860487759e-05\n",
      "398 7.23133998690173e-05\n",
      "399 7.011976413195953e-05\n",
      "400 6.798994581913576e-05\n",
      "401 6.592645513592288e-05\n",
      "402 6.392996874637902e-05\n",
      "403 6.199401832418516e-05\n",
      "404 6.011750883772038e-05\n",
      "405 5.829715519212186e-05\n",
      "406 5.653086554957554e-05\n",
      "407 5.481856351252645e-05\n",
      "408 5.316582246450707e-05\n",
      "409 5.155942926649004e-05\n",
      "410 5.00000205647666e-05\n",
      "411 4.8490477638551965e-05\n",
      "412 4.7028661356307566e-05\n",
      "413 4.561063178698532e-05\n",
      "414 4.423444261192344e-05\n",
      "415 4.2904081055894494e-05\n",
      "416 4.1618040995672345e-05\n",
      "417 4.036444806843065e-05\n",
      "418 3.914816988981329e-05\n",
      "419 3.797146928263828e-05\n",
      "420 3.683055911096744e-05\n",
      "421 3.572289278963581e-05\n",
      "422 3.464740075287409e-05\n",
      "423 3.361060953466222e-05\n",
      "424 3.260103039792739e-05\n",
      "425 3.1620955269318074e-05\n",
      "426 3.067159923375584e-05\n",
      "427 2.9757411539321765e-05\n",
      "428 2.8864953492302448e-05\n",
      "429 2.79975647572428e-05\n",
      "430 2.7161224352312274e-05\n",
      "431 2.6350555344833992e-05\n",
      "432 2.5560924768797122e-05\n",
      "433 2.4799408492981456e-05\n",
      "434 2.4058706912910566e-05\n",
      "435 2.333936936338432e-05\n",
      "436 2.2642367184744217e-05\n",
      "437 2.1965677660773508e-05\n",
      "438 2.1311601813067682e-05\n",
      "439 2.0677589418482967e-05\n",
      "440 2.005878195632249e-05\n",
      "441 1.946146585396491e-05\n",
      "442 1.888356746349018e-05\n",
      "443 1.8320646631764248e-05\n",
      "444 1.777455872797873e-05\n",
      "445 1.7246864445041865e-05\n",
      "446 1.6734695236664265e-05\n",
      "447 1.623589741939213e-05\n",
      "448 1.5753555999253877e-05\n",
      "449 1.5284958863048814e-05\n",
      "450 1.4833043678663671e-05\n",
      "451 1.4391095646715257e-05\n",
      "452 1.3965225662104785e-05\n",
      "453 1.3550794392358512e-05\n",
      "454 1.3151093298802152e-05\n",
      "455 1.2760622666974086e-05\n",
      "456 1.2383598004817031e-05\n",
      "457 1.201645045512123e-05\n",
      "458 1.166043512057513e-05\n",
      "459 1.1314971743558999e-05\n",
      "460 1.0980654224113096e-05\n",
      "461 1.0656813174136914e-05\n",
      "462 1.034320939652389e-05\n",
      "463 1.0036438652605284e-05\n",
      "464 9.739695087773725e-06\n",
      "465 9.451748155697715e-06\n",
      "466 9.17397665034514e-06\n",
      "467 8.902931767806876e-06\n",
      "468 8.639700354251545e-06\n",
      "469 8.387228263018187e-06\n",
      "470 8.139203600876499e-06\n",
      "471 7.900253876869101e-06\n",
      "472 7.666973942832556e-06\n",
      "473 7.4401855272299144e-06\n",
      "474 7.221431587822735e-06\n",
      "475 7.008947704889579e-06\n",
      "476 6.803440555813722e-06\n",
      "477 6.604882855754113e-06\n",
      "478 6.409892648662208e-06\n",
      "479 6.222149295354029e-06\n",
      "480 6.0386601035133936e-06\n",
      "481 5.860811597813154e-06\n",
      "482 5.6892322390922345e-06\n",
      "483 5.522489118447993e-06\n",
      "484 5.360537215892691e-06\n",
      "485 5.203985892876517e-06\n",
      "486 5.052397227700567e-06\n",
      "487 4.903388344246196e-06\n",
      "488 4.760309366247384e-06\n",
      "489 4.6212458073569e-06\n",
      "490 4.4855260057374835e-06\n",
      "491 4.354863449407276e-06\n",
      "492 4.227906174492091e-06\n",
      "493 4.103501396457432e-06\n",
      "494 3.984433533332776e-06\n",
      "495 3.868077328661457e-06\n",
      "496 3.7559370866802055e-06\n",
      "497 3.6455401186685776e-06\n",
      "498 3.539819772413466e-06\n",
      "499 3.4360298286628677e-06\n"
     ]
    }
   ],
   "source": [
    "#nn.Module 서브클래스 구현(ppt 18p)\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 2개의 nn.Linear 모듈을 생성하고, 멤버 변수로 지정합니다.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수에서는 입력 데이터의 Tensor를 받고 출력 데이터의 Tensor를\n",
    "        반환해야 합니다. Tensor 상의 임의의 연산자뿐만 아니라 생성자에서 정의한\n",
    "        Module도 사용할 수 있습니다.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 앞에서 정의한 클래스를 생성하여 모델을 구성합니다.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 손실 함수와 Optimizer를 만듭니다. SGD 생성자에 model.parameters()를 호출하면\n",
    "# 모델의 멤버인 2개의 nn.Linear 모듈의 학습 가능한 매개변수들이 포함됩니다.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 649.6943969726562\n",
      "1 623.2005615234375\n",
      "2 562.3232421875\n",
      "3 606.6149291992188\n",
      "4 614.536865234375\n",
      "5 582.6126098632812\n",
      "6 378.6232604980469\n",
      "7 609.0259399414062\n",
      "8 609.154541015625\n",
      "9 540.3336181640625\n",
      "10 272.0088195800781\n",
      "11 244.84213256835938\n",
      "12 605.7559814453125\n",
      "13 604.8405151367188\n",
      "14 490.4465026855469\n",
      "15 473.35748291015625\n",
      "16 121.1352767944336\n",
      "17 600.2303466796875\n",
      "18 579.6287231445312\n",
      "19 80.37773895263672\n",
      "20 70.96879577636719\n",
      "21 553.6882934570312\n",
      "22 331.7407531738281\n",
      "23 306.23486328125\n",
      "24 504.04998779296875\n",
      "25 242.55160522460938\n",
      "26 209.03497314453125\n",
      "27 96.0732421875\n",
      "28 95.39552307128906\n",
      "29 363.05596923828125\n",
      "30 72.1589584350586\n",
      "31 312.0638122558594\n",
      "32 410.1420593261719\n",
      "33 254.56735229492188\n",
      "34 221.73777770996094\n",
      "35 290.6672058105469\n",
      "36 120.99588012695312\n",
      "37 222.84620666503906\n",
      "38 194.68043518066406\n",
      "39 77.16815185546875\n",
      "40 142.4069366455078\n",
      "41 120.85267639160156\n",
      "42 154.43527221679688\n",
      "43 103.11292266845703\n",
      "44 72.10437774658203\n",
      "45 580.0013427734375\n",
      "46 383.6424255371094\n",
      "47 341.0157470703125\n",
      "48 147.59939575195312\n",
      "49 142.00978088378906\n",
      "50 145.0359344482422\n",
      "51 300.8743591308594\n",
      "52 427.74700927734375\n",
      "53 246.91500854492188\n",
      "54 336.5234375\n",
      "55 316.26806640625\n",
      "56 218.3426513671875\n",
      "57 350.14556884765625\n",
      "58 126.24974060058594\n",
      "59 188.42909240722656\n",
      "60 87.69013977050781\n",
      "61 246.0606689453125\n",
      "62 175.04196166992188\n",
      "63 105.37028503417969\n",
      "64 208.92501831054688\n",
      "65 69.54902648925781\n",
      "66 124.54532623291016\n",
      "67 117.79668426513672\n",
      "68 98.87771606445312\n",
      "69 102.01190948486328\n",
      "70 87.3718032836914\n",
      "71 76.7378921508789\n",
      "72 99.24545288085938\n",
      "73 142.85415649414062\n",
      "74 45.831031799316406\n",
      "75 89.79110717773438\n",
      "76 45.01815414428711\n",
      "77 84.89566802978516\n",
      "78 114.5448989868164\n",
      "79 95.39619445800781\n",
      "80 48.580379486083984\n",
      "81 70.84765625\n",
      "82 47.75551986694336\n",
      "83 73.03438568115234\n",
      "84 49.73697280883789\n",
      "85 42.65235900878906\n",
      "86 38.032470703125\n",
      "87 31.672266006469727\n",
      "88 25.66960906982422\n",
      "89 26.4572811126709\n",
      "90 19.045019149780273\n",
      "91 49.64756774902344\n",
      "92 17.89933967590332\n",
      "93 17.140169143676758\n",
      "94 17.765897750854492\n",
      "95 33.34909439086914\n",
      "96 15.186984062194824\n",
      "97 32.661685943603516\n",
      "98 28.763330459594727\n",
      "99 21.30534553527832\n",
      "100 13.837636947631836\n",
      "101 17.20904541015625\n",
      "102 37.4839973449707\n",
      "103 12.863808631896973\n",
      "104 18.98741912841797\n",
      "105 16.933963775634766\n",
      "106 12.906387329101562\n",
      "107 12.43921947479248\n",
      "108 17.059309005737305\n",
      "109 8.083489418029785\n",
      "110 6.79563045501709\n",
      "111 19.407867431640625\n",
      "112 15.49919605255127\n",
      "113 14.067867279052734\n",
      "114 13.710226058959961\n",
      "115 13.388284683227539\n",
      "116 7.476639747619629\n",
      "117 24.011869430541992\n",
      "118 8.931968688964844\n",
      "119 8.615571975708008\n",
      "120 23.524494171142578\n",
      "121 31.463830947875977\n",
      "122 9.153463363647461\n",
      "123 11.096832275390625\n",
      "124 73.65287780761719\n",
      "125 12.267951011657715\n",
      "126 4.421184062957764\n",
      "127 80.8996353149414\n",
      "128 3.901186943054199\n",
      "129 11.83508586883545\n",
      "130 3.1254327297210693\n",
      "131 42.87858200073242\n",
      "132 10.96546745300293\n",
      "133 4.286178112030029\n",
      "134 15.72054672241211\n",
      "135 29.182090759277344\n",
      "136 3.8845105171203613\n",
      "137 6.947582244873047\n",
      "138 20.325782775878906\n",
      "139 8.089600563049316\n",
      "140 21.677120208740234\n",
      "141 16.661840438842773\n",
      "142 9.70610523223877\n",
      "143 10.889827728271484\n",
      "144 3.984861135482788\n",
      "145 7.439220905303955\n",
      "146 8.518180847167969\n",
      "147 2.519436836242676\n",
      "148 5.239863395690918\n",
      "149 8.206766128540039\n",
      "150 3.2329866886138916\n",
      "151 5.388381004333496\n",
      "152 7.173079490661621\n",
      "153 6.193860054016113\n",
      "154 4.707026481628418\n",
      "155 3.141679048538208\n",
      "156 3.393671751022339\n",
      "157 3.3901050090789795\n",
      "158 9.231398582458496\n",
      "159 2.899282455444336\n",
      "160 7.410102844238281\n",
      "161 3.874180555343628\n",
      "162 1.5577441453933716\n",
      "163 1.1311103105545044\n",
      "164 0.9645703434944153\n",
      "165 2.802992343902588\n",
      "166 1.1842308044433594\n",
      "167 1.1750483512878418\n",
      "168 3.3068435192108154\n",
      "169 19.816144943237305\n",
      "170 4.046916961669922\n",
      "171 11.413302421569824\n",
      "172 8.16843032836914\n",
      "173 1.8136342763900757\n",
      "174 2.88203501701355\n",
      "175 4.114844799041748\n",
      "176 0.9586831331253052\n",
      "177 13.228227615356445\n",
      "178 5.004065990447998\n",
      "179 1.2102607488632202\n",
      "180 12.132182121276855\n",
      "181 6.06682014465332\n",
      "182 3.6363306045532227\n",
      "183 1.259608507156372\n",
      "184 2.7263550758361816\n",
      "185 3.398242712020874\n",
      "186 2.6300442218780518\n",
      "187 2.760115623474121\n",
      "188 1.6619793176651\n",
      "189 2.873267412185669\n",
      "190 12.805771827697754\n",
      "191 2.1973392963409424\n",
      "192 2.777871608734131\n",
      "193 4.305731773376465\n",
      "194 8.161134719848633\n",
      "195 1.322837471961975\n",
      "196 1.9478304386138916\n",
      "197 5.6447858810424805\n",
      "198 3.9026498794555664\n",
      "199 2.3327486515045166\n",
      "200 1.1834288835525513\n",
      "201 1.688880443572998\n",
      "202 1.5975723266601562\n",
      "203 6.4971513748168945\n",
      "204 3.379589796066284\n",
      "205 3.936763286590576\n",
      "206 1.2582052946090698\n",
      "207 6.332240581512451\n",
      "208 1.4752166271209717\n",
      "209 2.300596237182617\n",
      "210 3.035621166229248\n",
      "211 1.758363962173462\n",
      "212 2.297563314437866\n",
      "213 2.9135286808013916\n",
      "214 2.0126116275787354\n",
      "215 2.145423173904419\n",
      "216 0.881966769695282\n",
      "217 0.7795814871788025\n",
      "218 7.975690841674805\n",
      "219 1.7611974477767944\n",
      "220 2.587510585784912\n",
      "221 4.3791117668151855\n",
      "222 2.2792444229125977\n",
      "223 1.6374704837799072\n",
      "224 1.5216126441955566\n",
      "225 0.8183072209358215\n",
      "226 7.705583572387695\n",
      "227 12.12625503540039\n",
      "228 1.53254234790802\n",
      "229 12.105470657348633\n",
      "230 4.637174606323242\n",
      "231 4.702284336090088\n",
      "232 1.1385033130645752\n",
      "233 4.954850196838379\n",
      "234 16.733503341674805\n",
      "235 0.5620862245559692\n",
      "236 1.649627685546875\n",
      "237 0.8247148990631104\n",
      "238 16.51559829711914\n",
      "239 1.5239604711532593\n",
      "240 2.1335630416870117\n",
      "241 5.316394329071045\n",
      "242 3.4870097637176514\n",
      "243 1.6060845851898193\n",
      "244 2.294189929962158\n",
      "245 1.801912546157837\n",
      "246 0.43763282895088196\n",
      "247 0.4401979446411133\n",
      "248 0.48447540402412415\n",
      "249 2.912567615509033\n",
      "250 5.837007999420166\n",
      "251 1.3536239862442017\n",
      "252 1.5387563705444336\n",
      "253 2.016242265701294\n",
      "254 5.431815147399902\n",
      "255 2.257713794708252\n",
      "256 3.1103994846343994\n",
      "257 1.5696134567260742\n",
      "258 0.9575850963592529\n",
      "259 1.4092283248901367\n",
      "260 5.55993127822876\n",
      "261 1.4818443059921265\n",
      "262 2.575972318649292\n",
      "263 1.0030263662338257\n",
      "264 4.4719343185424805\n",
      "265 0.7197327017784119\n",
      "266 1.3590713739395142\n",
      "267 2.274768352508545\n",
      "268 2.052546501159668\n",
      "269 0.9620966911315918\n",
      "270 2.8972601890563965\n",
      "271 1.5886695384979248\n",
      "272 3.537687301635742\n",
      "273 1.6735424995422363\n",
      "274 1.689984917640686\n",
      "275 1.4166444540023804\n",
      "276 0.7377884984016418\n",
      "277 0.45101022720336914\n",
      "278 0.47189363837242126\n",
      "279 0.6585862040519714\n",
      "280 0.7730454206466675\n",
      "281 2.407890558242798\n",
      "282 5.931440353393555\n",
      "283 1.5192493200302124\n",
      "284 6.603620529174805\n",
      "285 6.366429805755615\n",
      "286 2.3518781661987305\n",
      "287 1.227196455001831\n",
      "288 1.8059667348861694\n",
      "289 20.493907928466797\n",
      "290 2.4241037368774414\n",
      "291 1.528274416923523\n",
      "292 3.3300955295562744\n",
      "293 12.87016487121582\n",
      "294 8.773001670837402\n",
      "295 2.646341562271118\n",
      "296 1.262986421585083\n",
      "297 6.009995460510254\n",
      "298 5.1180806159973145\n",
      "299 6.204169750213623\n",
      "300 2.909405469894409\n",
      "301 1.4399081468582153\n",
      "302 1.1714081764221191\n",
      "303 1.2649182081222534\n",
      "304 4.438797950744629\n",
      "305 3.637845993041992\n",
      "306 6.535538673400879\n",
      "307 1.0010374784469604\n",
      "308 6.309254169464111\n",
      "309 1.0233491659164429\n",
      "310 10.731833457946777\n",
      "311 8.001933097839355\n",
      "312 14.818863868713379\n",
      "313 2.7191593647003174\n",
      "314 3.56253981590271\n",
      "315 12.117518424987793\n",
      "316 13.001401901245117\n",
      "317 5.7005295753479\n",
      "318 9.458582878112793\n",
      "319 1.1622134447097778\n",
      "320 16.10148048400879\n",
      "321 14.835246086120605\n",
      "322 25.327905654907227\n",
      "323 2.706472873687744\n",
      "324 1.9120482206344604\n",
      "325 1.9982978105545044\n",
      "326 2.421560049057007\n",
      "327 1.789970874786377\n",
      "328 2.393716812133789\n",
      "329 1.969489574432373\n",
      "330 1.9845465421676636\n",
      "331 2.285149335861206\n",
      "332 2.0407216548919678\n",
      "333 2.7046051025390625\n",
      "334 1.3484920263290405\n",
      "335 1.280777931213379\n",
      "336 1.2509799003601074\n",
      "337 0.6665642261505127\n",
      "338 1.1471494436264038\n",
      "339 0.9459535479545593\n",
      "340 1.18109130859375\n",
      "341 3.0348634719848633\n",
      "342 0.7347744703292847\n",
      "343 2.416114091873169\n",
      "344 1.2493363618850708\n",
      "345 2.0419178009033203\n",
      "346 1.3727630376815796\n",
      "347 0.7599697709083557\n",
      "348 3.189532518386841\n",
      "349 2.1812589168548584\n",
      "350 0.6193062663078308\n",
      "351 0.525067150592804\n",
      "352 4.258843898773193\n",
      "353 0.35834628343582153\n",
      "354 3.104153633117676\n",
      "355 0.46993324160575867\n",
      "356 0.8109902739524841\n",
      "357 1.5340474843978882\n",
      "358 1.2011913061141968\n",
      "359 1.252329707145691\n",
      "360 0.8923478722572327\n",
      "361 0.9993582963943481\n",
      "362 1.4808424711227417\n",
      "363 0.6901413798332214\n",
      "364 1.6057220697402954\n",
      "365 0.8123559951782227\n",
      "366 1.1934181451797485\n",
      "367 1.0473462343215942\n",
      "368 1.3405817747116089\n",
      "369 0.5240774750709534\n",
      "370 0.7165318131446838\n",
      "371 0.9890406131744385\n",
      "372 0.9361776113510132\n",
      "373 0.28795328736305237\n",
      "374 0.18988817930221558\n",
      "375 0.13157162070274353\n",
      "376 0.7253852486610413\n",
      "377 0.8960802555084229\n",
      "378 0.1459471881389618\n",
      "379 0.14753767848014832\n",
      "380 0.13239982724189758\n",
      "381 0.6463928818702698\n",
      "382 1.3089842796325684\n",
      "383 1.3935983180999756\n",
      "384 0.6806866526603699\n",
      "385 0.9794380068778992\n",
      "386 1.398558259010315\n",
      "387 1.0248963832855225\n",
      "388 1.3579894304275513\n",
      "389 0.6496652960777283\n",
      "390 1.3449974060058594\n",
      "391 0.3241245746612549\n",
      "392 1.5829083919525146\n",
      "393 1.0061898231506348\n",
      "394 0.7220315337181091\n",
      "395 0.15221194922924042\n",
      "396 0.7605438828468323\n",
      "397 2.296217679977417\n",
      "398 0.10851359367370605\n",
      "399 0.6652161478996277\n",
      "400 0.7371290326118469\n",
      "401 0.7589597105979919\n",
      "402 0.6501160860061646\n",
      "403 0.14125537872314453\n",
      "404 0.9746025204658508\n",
      "405 0.23187613487243652\n",
      "406 0.8181324005126953\n",
      "407 0.5654549598693848\n",
      "408 0.7096548676490784\n",
      "409 0.6158484816551208\n",
      "410 0.8073509335517883\n",
      "411 0.270944744348526\n",
      "412 0.22831690311431885\n",
      "413 1.6275749206542969\n",
      "414 0.5618916153907776\n",
      "415 0.7669909596443176\n",
      "416 0.3397926688194275\n",
      "417 0.7536358833312988\n",
      "418 0.6918792724609375\n",
      "419 0.8598169088363647\n",
      "420 0.4114297330379486\n",
      "421 0.2571199834346771\n",
      "422 0.8346253037452698\n",
      "423 0.27247536182403564\n",
      "424 0.1856556087732315\n",
      "425 0.5070725083351135\n",
      "426 0.45459407567977905\n",
      "427 0.8411076068878174\n",
      "428 0.3699232339859009\n",
      "429 0.5945152044296265\n",
      "430 0.5626312494277954\n",
      "431 0.5267964005470276\n",
      "432 0.160691037774086\n",
      "433 0.5136432647705078\n",
      "434 0.1925429105758667\n",
      "435 0.5963405966758728\n",
      "436 0.555823802947998\n",
      "437 0.09944827854633331\n",
      "438 0.06765146553516388\n",
      "439 0.6085257530212402\n",
      "440 0.4168665111064911\n",
      "441 0.12323527038097382\n",
      "442 0.7626620531082153\n",
      "443 0.7922401428222656\n",
      "444 0.36101457476615906\n",
      "445 0.555443525314331\n",
      "446 0.7964051365852356\n",
      "447 0.48236310482025146\n",
      "448 0.5353104472160339\n",
      "449 0.5092200040817261\n",
      "450 0.4940173029899597\n",
      "451 0.39535513520240784\n",
      "452 0.4172920882701874\n",
      "453 0.3546748161315918\n",
      "454 0.18498662114143372\n",
      "455 0.501123309135437\n",
      "456 0.42007431387901306\n",
      "457 0.11198994517326355\n",
      "458 0.4605342745780945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459 0.2911495268344879\n",
      "460 0.06555371731519699\n",
      "461 0.5264791250228882\n",
      "462 0.29638439416885376\n",
      "463 0.4793793559074402\n",
      "464 0.40649768710136414\n",
      "465 0.3307681381702423\n",
      "466 0.11819446831941605\n",
      "467 0.4127310514450073\n",
      "468 0.237562358379364\n",
      "469 0.13416340947151184\n",
      "470 0.3369741141796112\n",
      "471 0.7944223284721375\n",
      "472 0.20597711205482483\n",
      "473 0.1268736571073532\n",
      "474 0.2079993039369583\n",
      "475 0.2655843496322632\n",
      "476 0.7054819464683533\n",
      "477 0.19209425151348114\n",
      "478 0.09609847515821457\n",
      "479 0.29017460346221924\n",
      "480 0.27799639105796814\n",
      "481 0.0713868960738182\n",
      "482 0.3015247881412506\n",
      "483 0.19422830641269684\n",
      "484 0.19090424478054047\n",
      "485 0.5544294714927673\n",
      "486 0.1736455261707306\n",
      "487 0.11361761391162872\n",
      "488 0.3384968042373657\n",
      "489 0.08753371983766556\n",
      "490 0.06968110054731369\n",
      "491 0.6881382465362549\n",
      "492 0.04396694153547287\n",
      "493 0.035219211131334305\n",
      "494 0.02936427854001522\n",
      "495 0.4673805236816406\n",
      "496 0.7014986872673035\n",
      "497 0.05879870802164078\n",
      "498 0.6912527084350586\n",
      "499 0.5719440579414368\n"
     ]
    }
   ],
   "source": [
    "#제어 흐름과 가중치 공유를 사용한 모델을 상속받는 서브클래스 구현(ppt 19p)\n",
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 순전파 단계에서 사용할 3개의 nn.Linear 인스턴스를 생성합니다.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        모델의 순전파 단계에서, 무작위로 0, 1, 2 또는 3 중에 하나를 선택하고\n",
    "        은닉층을 계산하기 위해 여러번 사용한 middle_linear Module을 재사용합니다.\n",
    "\n",
    "        각 순전파 단계는 동적 연산 그래프를 구성하기 때문에, 모델의 순전파 단계를\n",
    "        정의할 때 반복문이나 조건문과 같은 일반적인 Python 제어 흐름 연산자를 사용할\n",
    "        수 있습니다.\n",
    "\n",
    "        여기에서 연산 그래프를 정의할 때 동일 Module을 여러번 재사용하는 것이\n",
    "        완벽히 안전하다는 것을 알 수 있습니다. 이것이 각 Module을 한 번씩만 사용할\n",
    "        수 있었던 Lua Torch보다 크게 개선된 부분입니다.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 앞서 정의한 클래스를 생성(instantiating)하여 모델을 구성합니다.\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# 손실함수와 Optimizer를 만듭니다. 이 이상한 모델을 순수한 확률적 경사 하강법\n",
    "# (stochastic gradient decent)으로 학습하는 것은 어려우므로, 모멘텀(momentum)을\n",
    "# 사용합니다.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yonsei\\Anaconda3\\envs\\test\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.311689\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.302934\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.270443\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.261684\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.206999\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.170659\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.132587\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.026463\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.906063\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.692336\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.591767\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.304180\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.090737\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.811729\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.897568\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.793855\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.676824\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.609872\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.574376\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.544204\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.431459\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.537736\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.596436\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.410250\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.361692\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.327309\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.682536\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.573427\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.612797\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.418558\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.645928\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.364808\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.271998\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.435141\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.462801\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.537581\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.275257\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.311157\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.332986\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.253927\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.366805\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.330292\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.383493\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.468001\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.319162\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.532371\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.375028\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.291975\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.482294\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.249202\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.115300\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.175477\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.132354\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.460816\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.191218\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.261806\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.290919\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.326083\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.511959\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.315357\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.194907\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.110525\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.186042\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.159412\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.308471\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.271056\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.192041\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.199615\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.332383\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.243370\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.109719\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.123613\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.231395\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.163772\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.100155\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.149514\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.172809\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.103074\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.378223\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.125637\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.155006\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.307663\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.134249\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.270957\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.295784\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.331613\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.164255\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.265267\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.205631\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.121953\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.170639\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.265329\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.178138\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.426696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yonsei\\Anaconda3\\envs\\test\\lib\\site-packages\\ipykernel_launcher.py:72: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1834, Accuracy: 9421/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.238063\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.214220\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.126487\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.250875\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.065840\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.216428\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.111218\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.067835\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.168946\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.276814\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.133915\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.331594\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.190897\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.100960\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.128798\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.146906\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.165946\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.235767\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.216667\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.203453\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.213518\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.115132\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.193374\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.242084\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.124480\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.156505\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.246496\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.155639\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.105874\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.295519\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.070608\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.070394\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.254373\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.182623\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.146010\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.164435\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.157243\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.161577\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.080769\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.220327\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.126389\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.090938\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.131897\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.541123\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.167110\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.114671\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.079786\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.148654\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.108258\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.162759\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.143540\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.086421\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.074130\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.144126\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.173158\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.190966\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.189848\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.175420\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.120086\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.178230\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.152507\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.118924\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.117668\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.184021\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.212548\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.073241\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.276094\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.149390\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.191860\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.337988\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.131979\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.147956\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.122825\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.070409\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.199106\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.265351\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.116339\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.016018\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.093460\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.131892\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.213531\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.267232\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.146453\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.115459\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.095894\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.174104\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.129154\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.036754\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.067994\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.232411\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.137877\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.166209\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.192767\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.250646\n",
      "\n",
      "Test set: Average loss: 0.1166, Accuracy: 9657/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.066401\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.037813\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.227415\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.136231\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.106355\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.042364\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.053427\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.024181\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.142054\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.192832\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.229091\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.052728\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.143176\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.154536\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.049674\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.198293\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.259014\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.142370\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.040789\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.067062\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.070413\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.070192\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.103408\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.080892\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.139640\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.200603\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.131824\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.042227\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.169373\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.096448\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.200735\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.109010\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.175451\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.285306\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.060333\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.067548\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.126103\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.081977\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.033807\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.028269\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.091056\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.099021\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.060496\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.149701\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.134348\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.096484\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.203123\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.023702\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.081497\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.078576\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.157356\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.135023\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.120995\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.156437\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.230962\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.077929\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.205930\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.125863\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.216610\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.066337\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.156915\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.163910\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.186523\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.081448\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.094863\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.084856\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.044437\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.183984\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.093245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.089689\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.153056\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.031954\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.045373\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.157639\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.057031\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.107524\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.061340\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.056913\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.085665\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.222511\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.080041\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.306405\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.237521\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.089171\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.073375\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.087764\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.063674\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.165975\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.085388\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.117677\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.034545\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.108522\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.187026\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.117918\n",
      "\n",
      "Test set: Average loss: 0.0824, Accuracy: 9738/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.082146\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.051004\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.161148\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.117302\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.182868\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.126233\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.153616\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.069729\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.017304\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.185757\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.068991\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.035569\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.077364\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.079630\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.043478\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.186298\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.048673\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.114808\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.102849\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.183457\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.047957\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.094200\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.151896\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.037336\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.100547\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.141673\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.085370\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.067461\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.085462\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.031353\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.108800\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.186319\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.051259\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.060455\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.091221\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.036857\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.056294\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.027106\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.117068\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.061509\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.114718\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.065382\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.188483\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.024307\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.097690\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.070339\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.119547\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.132539\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.014201\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.027612\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.127247\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.095549\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.134884\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.307495\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.031794\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.095175\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.038978\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.093202\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.205602\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.105938\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.088809\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.070840\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.064082\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.027533\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.047733\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.083856\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.047214\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.097928\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.030412\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.118776\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.183006\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.139140\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.109754\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.082674\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.090900\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.078827\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.076800\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.051036\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.150365\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.241533\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.093121\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.041240\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.050693\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.125312\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.132169\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.109574\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.056021\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.141544\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.042535\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.127579\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.119752\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.333466\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.176883\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.068992\n",
      "\n",
      "Test set: Average loss: 0.0784, Accuracy: 9760/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.113841\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.090493\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.139896\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.065290\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.050802\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.076455\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.169822\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.094287\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.023949\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.044027\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.215060\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.087717\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.152109\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.059087\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.058346\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.088842\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.036091\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.079614\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.059629\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.139972\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.017200\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.169098\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.133882\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.159604\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.108338\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.075438\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.060773\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.117981\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.162692\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.124538\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.041336\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.027169\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.050313\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.072766\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.024078\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.060325\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.031628\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.109230\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.077465\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.157325\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.083120\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.033966\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.114298\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.065086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.134481\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.082512\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.027148\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.040457\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.110903\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.091112\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.095828\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.099219\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.026548\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.026641\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.030950\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.092064\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.039445\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.057687\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.103988\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.029450\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.020006\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.168025\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.142827\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.044022\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.069716\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.037232\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.062805\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.047983\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.070620\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.028351\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.079644\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.167124\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.035103\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.093344\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.036010\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.099827\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.083014\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.071837\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.070572\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.113022\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.127074\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.064061\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.059454\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.031608\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.127558\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.093439\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.047064\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.152193\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.216347\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.017520\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.152950\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.052738\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.114175\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.160742\n",
      "\n",
      "Test set: Average loss: 0.0637, Accuracy: 9799/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.113325\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.110089\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.129989\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.049237\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.022258\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.063311\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.131930\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.115445\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.068267\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.036208\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.024631\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.114813\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.047371\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.053550\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.038923\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.064963\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.131063\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.214547\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.066340\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.112198\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.112135\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.259654\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.075314\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.027256\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.075993\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.081915\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.054425\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.057340\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.108674\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.048561\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.033747\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.065008\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.056888\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.106369\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.025803\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.028511\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.078842\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.066823\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.036649\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.040055\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.053971\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.096007\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.030145\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.109549\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.038353\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.051661\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.041035\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.026601\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.011569\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.105224\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.069902\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.074545\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.014989\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.046895\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.034032\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.046899\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.011335\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.074695\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.037528\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.072442\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.101945\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.078692\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.038461\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.117751\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.028561\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.039333\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.011368\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.062964\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.121243\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.043673\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.148140\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.055315\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.089715\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.060781\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.086060\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.040422\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.026150\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.017864\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.038101\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.083073\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.089784\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.043543\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.026382\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.019662\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.037106\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.028855\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.064645\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.050372\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.037462\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.072915\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.025407\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.050778\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.032869\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.052576\n",
      "\n",
      "Test set: Average loss: 0.0632, Accuracy: 9812/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.069288\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.055125\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.134295\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.041706\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.029179\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.125835\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.011867\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.072105\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.005515\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.062045\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.053929\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.038847\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.050198\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.060224\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.077443\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.025678\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.079586\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.171907\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.116623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.114442\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.117073\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.047322\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.097983\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.066868\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.013467\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.113741\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.006359\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.040155\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.068984\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.068665\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.205616\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.035230\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.051930\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.063368\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.052641\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.070429\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.022124\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.027088\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.064714\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.109584\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.118452\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.012082\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.057470\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.067894\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.146101\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.067792\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.047904\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.062940\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.036752\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.064722\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.113571\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.201945\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.204558\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.097562\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.017972\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.018475\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.032892\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.034746\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.133026\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.155094\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.039433\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.079725\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.036276\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.154368\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.058953\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.046768\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.023119\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.021800\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.033906\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.047552\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.146948\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.134240\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.036061\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.031498\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.068327\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.076630\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.140801\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.064137\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.096733\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.122599\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.057306\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.060274\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.075092\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.033320\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.103711\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.040661\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.110265\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.037132\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.203015\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.014608\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.012348\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.071702\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.050435\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.015908\n",
      "\n",
      "Test set: Average loss: 0.0557, Accuracy: 9828/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.006047\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.132671\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.077600\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.061137\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.082564\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.021663\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.013191\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.066775\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.039247\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.052056\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.028075\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.045570\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.172307\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.054221\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.082427\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.015065\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.023452\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.101790\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.119153\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.029033\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.059949\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.011674\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.046044\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.058808\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.029965\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.090833\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.131776\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.077419\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.097017\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.210664\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.139455\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.033080\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.033851\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.017205\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.061336\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.019942\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.137948\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.013408\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.102194\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.073937\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.057889\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.010526\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.050549\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.137881\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.103153\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.050700\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.020741\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.007433\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.028026\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.096268\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.037731\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.031120\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.113536\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.011989\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.028219\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.051380\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.059702\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.062961\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.012978\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.155477\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.146727\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.088236\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.013976\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.023175\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.018456\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.057175\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.013035\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.012292\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.050363\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.125604\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.038607\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.025392\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.054427\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.074354\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.024325\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.134682\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.062059\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.012431\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.088958\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.022003\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.146738\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.077203\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.005360\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.079233\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.064655\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.143978\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.026248\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.087139\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.023849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.044340\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.109988\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.129914\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.073728\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.063756\n",
      "\n",
      "Test set: Average loss: 0.0521, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.117225\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.023546\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.072972\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.042843\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.029796\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.097720\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.015998\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.058185\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.269828\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.060827\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.022948\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.061768\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.010013\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.003400\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.024558\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.023340\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.023495\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.069052\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.124412\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.080877\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.037246\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.119059\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.035135\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.033588\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.069966\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.042562\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.037250\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.058760\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.085762\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.023516\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.007120\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.018356\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.073151\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.030613\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.007902\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.004011\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.053465\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.028403\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.129886\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.152613\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.031703\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.067858\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.186268\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.036863\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.050247\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.051729\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.049680\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.047125\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.019114\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.005972\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.077729\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.027600\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.022634\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.013177\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.031249\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.064210\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.198774\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.036600\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.087238\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.007269\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.023111\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.263094\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.044566\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.036264\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.028556\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.021341\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.087253\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.036724\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.008441\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.047162\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.055086\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.036472\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.091169\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.044760\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.038336\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.147604\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.162425\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.150577\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.026518\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.054381\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.024123\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.012983\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.080727\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.028619\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.050512\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.013561\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.018399\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.028733\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.079633\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.125900\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.025023\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.146509\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.174205\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.055568\n",
      "\n",
      "Test set: Average loss: 0.0591, Accuracy: 9817/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN 코드\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=25000 valid=25000\n",
      "[('the', 322198), ('a', 159953), ('and', 158572), ('of', 144462), ('to', 133967), ('is', 104171), ('in', 90527), ('i', 70480), ('this', 69714), ('that', 66292)]\n",
      "[(\"shite'\", 1), ('18,000', 1), ('whelk.<br', 1), ('chronicles.<br', 1), ('lascivious/decadent', 1), (\"me'...\", 1), ('american-canadian', 1), (\"'inferno'\", 1), (\"'irreversible'-style\", 1), ('intensity!', 1)]\n",
      "vocab_size=10002\n",
      "['<unk>', '<pad>', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'i']\n",
      "dict_keys(['neg', 'pos'])\n",
      "--------------------------------------------------------------------------------\n",
      "model params\n",
      "input_dim=10002, output=2\n",
      "n_layers=1, n_hid=256 embed=100\n",
      "batch=8\n",
      "Epoch 0/10 loss=0.634299971575737 acc=0.755079984664917 time=0:21:20.982472\n",
      "Epoch 1/10 loss=0.4012510151231289 acc=0.8662800192832947 time=0:18:50.133786\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-425d833d1ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;31m# do back propagation for bptt steps in time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#lstm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchtext import data, datasets\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from progress.bar import Bar\n",
    "\n",
    "torch.manual_seed(12)\n",
    "torch.cuda.manual_seed(12)\n",
    "np.random.seed(12)\n",
    "random.seed(12)\n",
    "\n",
    "USE_GPU=0\n",
    "\n",
    "device = torch.device('cuda' if (torch.cuda.is_available() and USE_GPU) else 'cpu')\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Simple tokenizer, change for something more sophisticated\n",
    "    \"\"\"\n",
    "    return text.lower().split()\n",
    "\n",
    "\n",
    "def accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    # apply softmax\n",
    "    preds = torch.nn.functional.softmax(preds, dim=1)\n",
    "    # get max values along rows\n",
    "    _, indices = preds.max(dim=1)\n",
    "    # values, indices = torch.max(tensor, 0)\n",
    "    correct = (indices == y).float()  # convert into float for division\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "##### Read the data\n",
    "\n",
    "# set up fields\n",
    "TEXT = data.Field(lower=True,\n",
    "                  include_lengths=True,\n",
    "                  tokenize=tokenize)\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "# make splits for data\n",
    "train_ds, valid_ds = datasets.IMDB.splits(TEXT, LABEL)\n",
    "# take a portion of datasets, for testing :)\n",
    "# train_ds, _ = train_ds.split(0.5)\n",
    "# valid_ds, _ = valid_ds.split(0.5)\n",
    "print(f'train={len(train_ds)} valid={len(valid_ds)}')\n",
    "\n",
    "# build the vocabulary\n",
    "TEXT.build_vocab(train_ds,\n",
    "                 min_freq=10,\n",
    "                 max_size=10000 ) #, vectors=GloVe(name='6B', dim=300))\n",
    "LABEL.build_vocab(train_ds)\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(10))\n",
    "print(TEXT.vocab.freqs.most_common()[:-11:-1])\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f'vocab_size={vocab_size}')\n",
    "print(list(vocab.stoi.keys())[0:10])\n",
    "\n",
    "print(LABEL.vocab.stoi.keys())\n",
    "\n",
    "#hidden size\n",
    "n_hid=256\n",
    "# embed size\n",
    "n_embed=100\n",
    "# number of layers\n",
    "n_layers=1\n",
    "batch_size = 8\n",
    "\n",
    "input_dim = vocab_size # =10002\n",
    "output_dim = len(LABEL.vocab) # =2\n",
    "\n",
    "train_iter = data.BucketIterator(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True, device=device)\n",
    "\n",
    "valid_iter = data.BucketIterator(\n",
    "    valid_ds, batch_size=batch_size,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True, device=device)\n",
    "\n",
    "\n",
    "print(\"-\"*80)\n",
    "print(f'model params')\n",
    "print(f'input_dim={input_dim}, output={output_dim}')\n",
    "print(f'n_layers={n_layers}, n_hid={n_hid} embed={n_embed}')\n",
    "print(f'batch={batch_size}')\n",
    "\n",
    "class SeqRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim,\n",
    "                 output_dim, embed_size,\n",
    "                 hidden_size, num_layers=1,\n",
    "                 dropout=0.1,vectors=None ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(input_dim, embed_size)\n",
    "        # if we want to copy embedding vectors\n",
    "        if vectors:\n",
    "            self.embed.weight.data.copy_(vectors)\n",
    "\n",
    "        #after the embedding we can add dropout\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size,\n",
    "                            num_layers, batch_first=False)\n",
    "        #output linear layer\n",
    "        self.linear = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # Embed word ids to vectors\n",
    "        len_seq, bs = seq.shape\n",
    "        w_embed = self.embed(seq)\n",
    "        w_embed = self.drop(w_embed)\n",
    "    \n",
    "        output, _ = self.rnn(w_embed)\n",
    "        \n",
    "        # this does .squeeze(0) now hidden has size [batch, hid dim]\n",
    "        last_output = output[-1, :, :]\n",
    "        # apply dropout\n",
    "        last_output = self.drop(last_output)\n",
    "\n",
    "        out = self.linear(last_output)\n",
    "        return out\n",
    "\n",
    "model = SeqRNN(input_dim=input_dim,\n",
    "               output_dim=output_dim,\n",
    "               embed_size=n_embed, hidden_size=n_hid)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "epoch_loss = 0\n",
    "epoch_acc = 0\n",
    "epoch = 10\n",
    "\n",
    "for e in range(epoch):\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    # train loop\n",
    "    model.train()\n",
    "    # progress\n",
    "    bar = Bar(f'Training Epoch {e}/{epoch}', max=len(train_iter))\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "\n",
    "        model.zero_grad()\n",
    "        # move data to device (GPU if enabled, else CPU do nothing)\n",
    "        batch_text = batch.text[0].to(device) # include lengths at [1]\n",
    "        batch_label = batch.label.to(device)\n",
    "        \n",
    "        predictions = model(batch_text)\n",
    "        # compute loss\n",
    "        loss = criterion(predictions, batch_label)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # do back propagation for bptt steps in time\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bar.next()\n",
    "\n",
    "    bar.finish()\n",
    "    # mean epoch loss\n",
    "    epoch_loss = epoch_loss / len(train_iter)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time\n",
    "\n",
    "    # progress\n",
    "    bar = Bar(f'Validation Epoch {e}/{epoch}', max=len(valid_iter))\n",
    "    # evaluation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_iter):\n",
    "            # print(f'batch_idx={batch_idx}')\n",
    "            batch_text = batch.text[0] #batch.text is a tuple\n",
    "            batch_label = batch.label\n",
    "            # get model output\n",
    "            predictions = model(batch_text)\n",
    "            # compute batch validation accuracy\n",
    "            acc = accuracy(predictions, batch_label)\n",
    "\n",
    "            epoch_acc += acc\n",
    "            bar.next()\n",
    "\n",
    "    epoch_acc = epoch_acc/len(valid_iter)\n",
    "    bar.finish()\n",
    "\n",
    "    # show summary\n",
    "    print(\n",
    "        f'Epoch {e}/{epoch} loss={epoch_loss} acc={epoch_acc} time={time_elapsed}')\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "module.layer1.fc1.weight\n",
      "module.layer1.fc1.bias\n",
      "module.layer1.bn1.weight\n",
      "module.layer1.bn1.bias\n",
      "module.layer1.bn1.running_mean\n",
      "module.layer1.bn1.running_var\n",
      "module.layer1.bn1.num_batches_tracked\n",
      "module.layer2.fc2.weight\n",
      "module.layer2.fc2.bias\n",
      "tensor(0.2942, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5929, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0th epoch gen_loss: 0.29424265027046204 dis_loss: 0.5928618311882019\n",
      "tensor(0.2947, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0th epoch gen_loss: 0.29465562105178833 dis_loss: 0.5182761549949646\n",
      "tensor(0.2924, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "1th epoch gen_loss: 0.2924443483352661 dis_loss: 0.5084043741226196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-953f9eb649b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;31m# 랜덤한 z를 샘플링해줍니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mgen_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mdis_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-953f9eb649b1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     51\u001b[0m         ]))\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1655\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1656\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1657\u001b[0m     )\n\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GAN\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "import torchvision.utils as v_utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "# 참고\n",
    "# 전치 컨볼루션 연산으로 이미지 크기를 2배로 늘리는 방법 2가지\n",
    "# 둘중에 kernel_size=4,stride=2,padding=1 세팅이 체커보드 아티팩트가 덜합니다.\n",
    "\n",
    "test = torch.ones(1,1,16,16)\n",
    "conv1 = nn.ConvTranspose2d(1,1,kernel_size=4,stride=2,padding=1)\n",
    "out = conv1(test)\n",
    "\n",
    "conv1 = nn.ConvTranspose2d(1,1,kernel_size=3,stride=2,padding=1,output_padding=1)\n",
    "out = conv1(test)\n",
    "\n",
    "epoch = 50\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "num_gpus = 1\n",
    "z_size = 50\n",
    "middle_size = 200\n",
    "\n",
    "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "# Generator receives random noise z and create 1x28x28 image\n",
    "# OrderedDict를 사용해 해당 연산의 이름을 지정할 수 있습니다.\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "                        ('fc1',nn.Linear(z_size,middle_size)),\n",
    "                        ('bn1',nn.BatchNorm1d(middle_size)),\n",
    "                        ('act1',nn.ReLU()),\n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "                        ('fc2', nn.Linear(middle_size,784)),\n",
    "                        #('bn2', nn.BatchNorm1d(784)),\n",
    "                        ('tanh', nn.Tanh()),\n",
    "        ]))\n",
    "    def forward(self,z):\n",
    "        out = self.layer1(z)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(batch_size,1,28,28)\n",
    "        return out\n",
    "\n",
    "# Discriminator receives 1x28x28 image and returns a float number 0~1\n",
    "# we can name each layer using OrderedDict\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "                        ('fc1',nn.Linear(784,middle_size)),\n",
    "                        #('bn1',nn.BatchNorm1d(middle_size)),\n",
    "                        ('act1',nn.LeakyReLU()),  \n",
    "            \n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "                        ('fc2', nn.Linear(middle_size,1)),\n",
    "                        ('bn2', nn.BatchNorm1d(1)),\n",
    "                        ('act2', nn.Sigmoid()),\n",
    "        ]))\n",
    "                                    \n",
    "    def forward(self,x):\n",
    "        out = x.view(batch_size, -1)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "    \n",
    "# Put class objects on Multiple GPUs using \n",
    "# torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n",
    "# device_ids: default all devices / output_device: default device 0 \n",
    "# along with .cuda()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "generator = nn.DataParallel(Generator()).to(device)\n",
    "discriminator = nn.DataParallel(Discriminator()).to(device)\n",
    "\n",
    "# Get parameter list by using class.state_dict().keys()\n",
    "\n",
    "gen_params = generator.state_dict().keys()\n",
    "dis_params = discriminator.state_dict().keys()\n",
    "\n",
    "for i in gen_params:\n",
    "    print(i)\n",
    "\n",
    "# loss function, optimizers, and labels for training\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "gen_optim = torch.optim.Adam(generator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
    "dis_optim = torch.optim.Adam(discriminator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
    "\n",
    "ones_label = torch.ones(batch_size,1).to(device)\n",
    "zeros_label = torch.zeros(batch_size,1).to(device)\n",
    "\n",
    "# train\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j,(image,label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        \n",
    "        # 구분자 학습\n",
    "        dis_optim.zero_grad()\n",
    "      \n",
    "        # Fake Data \n",
    "        # 랜덤한 z를 샘플링해줍니다.\n",
    "        z = init.normal_(torch.Tensor(batch_size,z_size),mean=0,std=0.1).to(device)\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        # Real Data\n",
    "        dis_real = discriminator.forward(image)\n",
    "        \n",
    "        # 두 손실을 더해 최종손실에 대해 기울기 게산을 합니다.\n",
    "        dis_loss = torch.sum(loss_func(dis_fake,zeros_label)) + torch.sum(loss_func(dis_real,ones_label))\n",
    "        dis_loss.backward(retain_graph=True)\n",
    "        dis_optim.step()\n",
    "        \n",
    "        # 생성자 학습\n",
    "        gen_optim.zero_grad()\n",
    "        \n",
    "        # Fake Data\n",
    "        z = init.normal_(torch.Tensor(batch_size,z_size),mean=0,std=0.1).to(device)\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        gen_loss = torch.sum(loss_func(dis_fake,ones_label)) # fake classified as real\n",
    "        gen_loss.backward()\n",
    "        gen_optim.step()\n",
    "    \n",
    "        # model save\n",
    "        if j % 100 == 0:\n",
    "            print(gen_loss,dis_loss)\n",
    "            torch.save([generator,discriminator],'./model/vanilla_gan.pkl')            \n",
    "            v_utils.save_image(gen_fake.cpu().data[0:25],\"./result/gen_{}_{}.png\".format(i,j), nrow=5)\n",
    "            print(\"{}th epoch gen_loss: {} dis_loss: {}\".format(i,gen_loss.data,dis_loss.data))\n",
    "            \n",
    "from glob import glob \n",
    "\n",
    "for i in range(epoch):\n",
    "  print(i)\n",
    "  file_list = glob(\"./result/gen_{}_*.png\".format(i))\n",
    "  img_per_epoch = len(file_list)\n",
    "  for idx,j in enumerate(file_list):\n",
    "    img = plt.imread(j)\n",
    "    plt.subplot(1,img_per_epoch,idx+1)\n",
    "    plt.imshow(img)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
